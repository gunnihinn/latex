\documentclass[10pt,leqno]{article}

\usepackage{lmodern}
%\linespread{1.1}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[normalem]{ulem}
\usepackage{textcomp}
\usepackage{bookmark}
\usepackage{tikz-cd}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

% Note: The original text is out of copyright:
% The original text was published in 1944.
% Artin died in 1962, at which time US copyright lasted for 28 years after publication, meaning the text became public domain in 1972.
% Copyright was only extended in 1972, after the original had become public domain.
% https://en.wikipedia.org/wiki/Copyright_law_of_the_United_States#History

\author{Emil Artin}
\date{\today}
\title{Galois theory}

\newtheorem{theo}{Theorem}
\newtheorem*{theo*}{Theorem}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{lemm}{Lemma}
\newtheorem*{lemm*}{Lemma}
\newtheorem{coro}{Corollary}
\newtheorem*{coro*}{Corollary}
\theoremstyle{definition}
\newtheorem{defi}[theo]{Definition}
\newtheorem*{defi*}{Definition}
\newtheorem{exam}[theo]{Example}
\newtheorem*{rema}{Remark}

\newcommand{\kk}[1]{\mathbb{#1}}
\newcommand{\cc}[1]{\mathcal{#1}}

\def\eps{\varepsilon}
\def\empty{\varnothing}

\def\ov#1{\overline{#1}}

\def\CC{\mathbf{C}}
\def\EE{\mathcal{E}}
\def\FF{\mathcal{F}}
\def\NN{\mathbf{N}}
\def\RR{\mathbf{R}}
\def\QQ{\mathbf{Q}}
\def\ZZ{\mathbf{Z}}
\def\PP{\mathbf{P}}

\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\ord}{ord}

\renewcommand\thesection{\Roman{section}}
\renewcommand\thesubsection{\Alph{subsection}.}

\begin{document}

\maketitle



\section{Linear algebra}


\subsection{Fields}


A field is a set of elements in which a pair of operations called multiplication and addition is defined analogous to the operations of multiplication and addition in the real number system (which is itself an example of a field).
In each field $F$ there exist unique elements called 0 and 1 which, under the operations of addition and multiplication, behave with respect to all the other elements of $F$ exactly as their correspondents in the real number system.
In two respects, the analogy is not complete: 1) multiplication is not assumed to be commutative in every field, and 2) a field may have only a finite number of elements.

More exactly, a field is a set of elements which, under the above mentioned operation of addition, forms an additive abelian group and for which the elements, exclusive of zero, form a multiplicative group and, finally, in which the two group operations are connected by the distributive law.
Furthermore, the product of 0 and any element is defined to be 0.

If multiplication in the field is commutative, then the field is called a commutative field.


\subsection{Vector spaces}

If $V$ is an additive abelian group with elements $A$, $B$, $\ldots$, $F$ a field with elements $a$, $b$, $\ldots$, and if for each $a \in F$ and $A \in V$ the product $aA$ denotes an element of $V$, then $V$ is called a \emph{(left) vector space over $F$} if the following assumptions hold:

\begin{enumerate}
\item
a(A + B) = aA + aB

\item
(a + b)A = aA + bA

\item
a(bA) = (ab) A

\item
1A = A
\end{enumerate}

The reader may readily verify that if $V$ is a vector space over $F$, then $oA = 0$ and $a0 = 0$ where $o$ is the zero element of $F$ and $0$ that of $V$.
For example, the first relation follows from the equations:
\[
aA = (a + o)A = aA + oA
\]

Sometimes product between elements of $F$ and $V$ are written in the form $Aa$ in which case $V$ is called a \emph{right vector space over $F$} to distinguish it from the previous case where multiplication by field elements is from the left.
If, in the discussion, left and right vector spaces do not occur simultaneously, we shall simply use the term ``vector space''.


\subsection{Homogeneous linear equations}

If in a field $F$, $a_{ij}$, $i = 1,2,\ldots,m$, $j = 1,2,\ldots,n$ are $m \cdot n$ elements, it is frequently necessary to know conditions guaranteeing the existence of elements in $F$ such that the following equations are satisfied:
\begin{equation}
\label{eq:on}
\begin{aligned}
a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n &= 0,
\\
\vdots &
\\
a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn} x_n &= 0.
\end{aligned}
\end{equation}
The reader will recall that such equations are called \emph{linear homogeneous equations}, and a set of elements $x_1, x_2, \ldots, x_n$ of $F$, for which all the above equations are true, is called a solution of the system.
If not all of the elements $x_1, x_2, \ldots, x_n$ are $0$ the solution is called \emph{non-trivial}; otherwise it is called \emph{trivial}.


\begin{theo}
\label{theo:on}
A system of linear homogeneous equations always as a non-trivial solution if the number of unknowns exceeds the number of equations.
\end{theo}


The proof of this follows the method familiar to most high school students, namely, successive elimination of unknowns.
If no equations in $n > 0$ variables are prescribed, then our unknowns are unrestricted and we may set them all $= 1$.

We shall proceed by complete induction.
Let us suppose that each system of $k$ equations in more than $k$ unknowns has a non-trivial solution when $k < m$.
In the system of equations~\eqref{eq:on} we assume that $n > m$, and denote the expression $a_{i1} x_1 + \ldots + a_{in} x_n$ by $L_i$, $i = 1,2,\ldots,m$.
We seek elements $x_1, \ldots, x_n$ not all $0$ such that $L_1 = L_2 = \cdots = L_m = 0$.
If $a_{ij} = 0$ for each $i$ and $j$, then any choice of $x_1, \ldots, x_n$ will serve as a solution.
If not all $a_{ij}$ are 0, then we may assume that $a_{11} \not= 0$, for the order in which the equations are written or in which the unknowns are numbered has no influence on the existence of non-existence of a simultaneous solution.
We can find a non-trivial solution to our given system of equations, if and only if we can find a non-trivial solution to the following system:
\begin{align*}
L_1 &= 0
\\
L_2 - a_{21} a_{11}^{-1} L_1 &= 0
\\
&\vdots
\\
L_m - a_{m1} a_{11}^{-1} L_1 &= 0
\end{align*}
For, if $x_1, \ldots, x_n$ is a solution of these latter equations then, since $L_1 = 0$, the second term in each of the remaining equations is 0 and, hence, $L_2 = L_3 = \cdots = L_m = 0$.
Conversely, if \eqref{eq:on} is satisfied, then the new system is clearly satisfied.
The reader will notice that the new system was set up in such a way as to ``eliminate'' $x_1$ from the last $m-1$ equations.
Furthermore, if a non-trivial solution of the last $m-1$ equations, when viewed as equations in $x_2, \ldots, x_n$, exists then taking $x_1 = -a_{11}^{-1}(a_{12} x_2 + a_{13} x_3 + \cdots + a_{1n} x_n)$ would give us a solution to the whole system.
However, the last $m-1$ equations have a solution by our inductive assumption, from which the theorem follows.


\begin{rema}
If the linear homogeneous equations had been written in the form $\sum x_j a_{ij} = 0$, $j = 1, 2, \ldots, n$, the above theorem would still hold and with the same proof although with the order in which terms are written changed in a few instances.
\end{rema}



\subsection{Dependence and independence of vectors}

In a vector space $V$ over a field $F$, the vectors $A_1, \ldots, A_n$ are called \emph{dependent} if there exist elements $x_1, \ldots, x_n$, not all $0$, of $F$ such that $x_1 A_1 + x_2 A_2 + \cdots + x_n A_n = 0$.
If the vectors $A_1, \ldots A_n$ are not dependent, they are called \emph{independent}.

The \emph{dimension} of a vector space $V$ over a field $F$ is the maximum number of independent elements in $V$. Thus, the dimension of $V$ is $n$ if there are $n$ independent elements in $V$, but no set of more than $n$ independent elements.

A system $A_1, \ldots, A_m$ of elements in $V$ is called a \emph{generating system} of $V$ if each element $A$ of $V$ can be expressed linearly in terms of $A_1, \ldots, A_m$, i.e., $A = \sum_{i = 1}^m a_i A_i$ for a suitable choice of $a_i$, $i = 1,\ldots, m$, in $F$.


\begin{theo}
\label{theo:tw}
In any generating system the maximum number of independent vectors is equal to the dimension of the vector space.
\end{theo}

Let $A_1, \ldots, A_m$ be a generating system of a vector space $V$ of dimension $n$.
Let $r$ be the maximum number of independent elements in the generating system.
By a suitable reordering of the generators we may assume $A_1, \dots, A_r$ independent.
By the definition of dimension it follows that $r \leq n$.
For each $j$, $A_1, \ldots, A_r, A_{r+j}$ are dependent, and in the relation
\[
a_1 A_1 + a_2 A_2 + \cdots + a_r A_r + a_{r+j} A_{r+j} = 0
\]
expressing this, $a_{r+j} \not=0$, for the contrary would assert the dependence of $A_1, \ldots, A_r$.
Thus,
\[
A_{r+j} = -a_{r+j}^{-1}(a_1 A_1 + a_2 A_2 + \cdots + a_r A_r).
\]
It follows that $A_1, \ldots, A_r$ is also a generating system since in the linear relation for any element of $V$ the terms involving $A_{r+j}$, $j\not=0$, can all be replaced by linear expressions in $A_1, \ldots, A_r$.

Now, let $B_1, \ldots B_t$ be any system of vectors in $V$ where $t > r$, then there exists $a_{ij}$ such that $B_j = \sum_{i=1}^r a_{ij} A_i$, $j=1,2,\dots,t$, since the $A_i$'s form a generating system.
If we can show that $B_1, \ldots, B_t$ are dependent, this will give us $r \geq n$, and the theorem will follow from this together with the previous inequality $r \leq n$.
Thus, we must exhibit the existence of a non-trivial solution of $F$ of the equation
\[
x_1 B_1 + x_2 B_2 + \cdots + x_t B_t = 0.
\]
To this end, it will be sufficient to choose the $x_i$'s so as to satisfy the linear equations $\sum_{j=1}^t x_j a_{ij} = 0$, $i = 1,2,\dots,r$, since these expressions will be the coefficients of $A_i$ when in $\sum_{j=1}^t x_j B_j$ the $B_j$'s are replaced by $\sum_{i=1}^r a_{ij} A_i$ and terms are collected.
A solution to the equations $\sum_{j=1}^t x_j a_{ij} = 0$, $i = 1,2,\ldots,r$ always exists by Theorem~\ref{theo:on}.


\begin{rema}
Any $n$ independent vectors $A_1, \ldots, A_n$ in an $n$ dimensional vector space form a generating system.
For any vector $A$, the vectors $A, A_1, \ldots, A_n$ are dependent and the coefficient of $A$, in the dependence relation, cannot be zero.
Solving for $A$ in terms of $A_1, \ldots, A_n$, exhibits $A_1, \ldots, A_n$ as a generating system.
\end{rema}


A subset of a vector space is called a \emph{subspace} if it is a subgroup of the vector space and if, in addition, the multiplication of any element in the subset by any element in the field is also in the subset.
If $A_1, \ldots, a_n$ are elements on a vector space $V$, then the set of all elements of the form $a_1 A_1 + \ldots + a_n A_n$ clearly forms a subspace of $V$.
It is also evident, from the definition of dimension, that the dimension of any subspace never exceeds the dimension of the whole vector space.

An $s$-tuple of elements $(a_1, \ldots, a_s)$ in a field $F$ will be called a \emph{row vector}.
The totality of such $s$-tuple for a vector space if we define

$\alpha$)
$(a_1, a_2, \ldots, a_s) = (b_1, b_2, \ldots, b_s)$ if and only if $a_i = b_i$, $i = 1, \ldots, s$,

$\beta$)
$(a_1, a_2, \ldots, a_s) + (b_1, b_2, \ldots, b_s)
= (a_1 + b_1, a_2 + b_2, \ldots, a_s + b_2)$,

$\gamma$)
$b (a_1, a_2, \ldots, a_s) = (ba_1, ba_2, \ldots, ba_s)$, for $b$ an element of $F$.

When the $s$-tuples are written vertically,
\[
\begin{pmatrix}
a_1 \\ \vdots \\ a_s
\end{pmatrix}
\]
they will be called \emph{column vectors}.


\begin{theo}
\label{theo:th}
The row (column) vector space $F^n$ of all $n$-tuples from a field $F$ is a vector space of dimension $n$ over $F$.
\end{theo}

The $n$ elements
\begin{align*}
\eps_1 &= (1, 0, 0, \ldots, 0)
\\
\eps_2 &= (0, 1, 0, \ldots, 0)
\\
       & \vdots 
\\
\eps_n &= (0, 0, 0, \ldots, 1)
\end{align*}
are independent and generate $F^n$, as
%Both remarks follow from the relation 
$(a_1, a_2, \ldots, a_n) = \sum a_i \eps_i$.


We call a rectangular array
\[
\begin{pmatrix}
a_{11} & a_{12} & \ldots & a_{1n}
\\
a_{21} & a_{22} & \ldots & a_{2n}
\\
\vdots & \vdots & \ddots & \vdots
\\
a_{m1} & a_{m2} & \ldots & a_{mn}
\end{pmatrix}
\]
of elements of a field $F$ a \emph{matrix}.
By the \emph{right row rank} of a matrix, we mean the maximum number of independent row vectors among the rows $(a_{i1}, \ldots, a_{in})$ of the matrix when multiplication by field elements is from the right.
Similarly, we define left row rank, right column rank and left column rank.


\begin{theo}
\label{theo:fo}
In any matrix the right column rank equals the left row rank and the left column rank equals the right row rank.
If the field is commutative, these four numbers are equal to each other and are called the rank of the matrix.
\end{theo}


Call the column vectors of the given matrix $C_1, \ldots, C_n$ and the row vectors $R_1, \ldots, R_m$.
The column vector $0$ is
\[
\begin{pmatrix}
0 \\ 0 \\ \vdots \\ 0
\end{pmatrix}
\]
and any dependence $C_1 x_1 + C_2 x_2 + \cdots + C_n x_n = 0$ is equivalent to a solution of the equations
\begin{equation}
\label{eq:Don}
\begin{aligned}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= 0
\\
\vdots &
\\
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= 0.
\end{aligned}
\end{equation}
Any change in the order in which the rows of the matrix are written gives rise to the same system of equations and, hence, does not change the column rank of the matrix, but also does not change the row rank since the changed matrix would have the same set of row vectors.
Call $c$ the right column rank and $r$ the left row rank of the matrix.
By the above remarks we may assume that the first $r$ rows are independent row vectors.
The row vector space generated by all the rows of the matrix has, by Theorem~\ref{theo:on}, the dimension $r$ and is even generated by the first $r$ rows.
Thus, each row after the $r^{\text{th}}$ is linearly expressible in terms of the first $r$ rows.
Consequently, any solution of the first $r$ equations in \eqref{eq:Don} will be a solution of the entire system since any of the last $n-r$ equations is obtainable as a linear combination of the first $r$.
Conversely, any solution of \eqref{eq:Don} will also be a solution of the first $r$ equations.
This means that the matrix
\[
\begin{pmatrix}
a_{11} & a_{12} & \ldots & a_{1n}
\\
a_{21} & a_{22} & \ldots & a_{2n}
\\
\vdots & \vdots & \ddots & \vdots
\\
a_{r1} & a_{r2} & \ldots & a_{rn}
\end{pmatrix}
\]
consisting of the first $r$ rows of the original matrix has the same right column rank as the original.
It also has the same left row rank since the $r$ rows were chosen independent.
But the column rank of the amputated matrix cannot exceed $r$ by Theorem~\ref{theo:th}.
Hence, $c \leq r$.
Similarly, calling $c'$ the left column rank and $r'$ the right column rank, $c' \leq r'$.

If we form the transpose of the original matrix, that is, replace rows by columns and columns by rows, then the left row rank of the transposed matrix equals the left column rank of the original.
If then to the transposed matrix we apply the above considerations we arrive at $r \leq c$ and $r' \leq c'$.



\subsection{Non-homogeneous linear equations}

The system of non-homogeneous linear equations
\begin{equation}
\label{eq:Etw}
\begin{aligned}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1
\\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2
\\
\vdots &
\\
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= b_m
\end{aligned}
\end{equation}
has a solution if and only if the column vector
\[
\begin{pmatrix}
b_1 \\ \vdots \\ b_m
\end{pmatrix}
\]
lies in the space generated by the vectors
\[
\begin{pmatrix}
a_{11} \\ \vdots \\ a_{m1}
\end{pmatrix}
,\quad \ldots,\quad 
\begin{pmatrix}
a_{1n} \\ \vdots \\ a_{mn}
\end{pmatrix}.
\]
This means that there is a solution if and only if the right column rank of the matrix
\[
\begin{pmatrix}
a_{11} & \cdots & a_{1n}
\\
\vdots & \ddots & \vdots
\\
a_{m1} & \cdots & a_{mn}
\end{pmatrix}
\]
is the same as the right column rank of the augmented matrix
\[
\begin{pmatrix}
a_{11} & \cdots & a_{1n} & b_1
\\
\vdots & \ddots & \vdots & \vdots
\\
a_{m1} & \cdots & a_{mn} & b_m
\end{pmatrix}
\]
since the vector space generated by the original matrix must be the same as the vector space generated by the augmented matrix and in either case the dimension is the same as the rank of the matrix by Theorem~\ref{theo:tw}.

By Theorem~\ref{theo:fo}, this means that the row ranks are equal.
Conversely, if the row rank of the augmented matrix is the same as the row rank of the original matrix, the column ranks will be the same and the equations will have a solution.

If the equations~\eqref{eq:Etw} have a solution, then any relation among the rows of the original matrix subsists among the rows of the augmented matrix.
For equations~\eqref{eq:Etw} this merely means that like combinations of equals are equal.
Conversely, if each relation which subsists between the rows of the augmented matrix, then the row rank of the augmented matrix is the same as the row rank of the original matrix.
\emph{In terms of the equations this means that there will exist a solution if and only if the equations are consistent, i.e., if and only if any dependence between the left hand sides of the equations also holds between the right sides.}


\begin{theo}
\label{theo:fi}
If in equations~\eqref{eq:Etw} $m = n$, there exists a unique solution if and only if the corresponding homogeneous equations
\begin{align*}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= 0
\\
\vdots &
\\
a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n &= 0
\end{align*}
have only the trivial solution.
\end{theo}


If they have only the trivial solution, then the column vectors are independent.
It follows that the original $n$ equations in $n$ unknowns will have a unique solution if they have any solution, since the difference, term by term, of two distinct solutions would be a non-trivial solution of the homogeneous equations.
A solution would exist since the $n$ independent column vectors form a generating system for the $n$-dimensional space of column vectors.

Conversely, let us suppose our equations have one and only one solution.
In this case, the homogeneous equations added term by term to a solution of the original equations would yield a new solution to the original equations.
Hence, the homogeneous equations have only the trivial solution.


\subsection{Determinants}

The theory of determinants that we shall develop in this chapter is not need\-ed in Galois theory.
The reader may, therefore, omit this section if he so desires.%
\footnote{Of the preceding theory only Theorem~\ref{theo:on}, for homogeneous equations and the notion of linear dependence are assumed known.}

We assume our field to be commutative and consider the square matrix
\begin{equation}
\label{eq:1F1}
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n}
\\
a_{21} & a_{22} & \cdots & a_{2n}
\\
\cdots & \cdots & \cdots & \cdots
\\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix}
\end{equation}
of $n$ rows and $n$ columns.
We shall define a certain function of this matrix whose value is an element of our field.
The function will be called the determinant and will be denoted by
\begin{equation}
\label{eq:1F2}
\begin{vmatrix}
a_{11} & a_{12} & \cdots & a_{1n}
\\
a_{21} & a_{22} & \cdots & a_{2n}
\\
\cdots & \cdots & \cdots & \cdots
\\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{vmatrix}
\end{equation}
or by $D(A_1, A_2, \ldots, A_n)$ if we wish to consider it as a function of the column vectors $A_1, A_2, \ldots, A_n$ of \eqref{eq:1F1}.
if we keep all the columns but $A_k$ constant and consider the determinant as a function of $A_k$, then we write $D_k(A_k)$ and sometimes even only $D$.

\begin{defi*}
A function of the column vectors is a determinant if it satisfies the following three axioms:
\begin{enumerate}
\item 
\label{ax:1}
Viewed as a function of any column $A_k$ it is linear and homonegenous, i.e.,
\begin{align}
\label{eq:1F3}
D_k(A_k + A_k') &= D_k(A_k) + D_k(A_k'),
\\
\label{eq:1F4}
D_k(c A_k) &= c \cdot D_k(A_k).
\end{align}

\item
\label{ax:2}
Its value is $=0$\footnote{Henceforth, $0$ will denote the zero element of a field.} if the adjancent columns $A_k$ and $A_{k+1}$ are equal.

\item
\label{ax:3}
Its value is $=1$ if all the $A_k$ are the unit vectors $U_k$ where
\begin{equation}
\label{eq:1F5}
U_1 = \begin{pmatrix}
1 \\
0 \\
0 \\
\vdots \\
0
\end{pmatrix},
\quad
U_2 = \begin{pmatrix}
0 \\
1 \\
0 \\
\vdots \\
0
\end{pmatrix},
\quad
\ldots, 
\quad
U_n = \begin{pmatrix}
0 \\
0 \\
0 \\
\vdots \\
1
\end{pmatrix}.
\end{equation}
\end{enumerate}
\end{defi*}


The question as to whether determinants exist will be left open for the present.
But we derive consequences from the axioms:


a) If we put $c = 0$ in \eqref{eq:1F4} we get: a determinant is $0$ if one of the columns is $0$.

b) $D_k(A_k) = D_k(A_k + c A_{k+1})$ or a determinant remains unchanged if we add a multiple of one column to an adjacent column. Indeed
\[
D_k(A_k + c A_{k+1})
= D_k(A_k) + c D_k(A_{k+1})
= D_k(A_k)
\]
because of axiom \ref{ax:2}.

c) Consider the two columns $A_k$ and $A_{k+1}$.
We may replace them by $A_{k}$ and $A_{k+1} + A_k$; subtracting the second from the first we may replace them by $-A_{k+1}$ and $A_{k+1} + A_k$; adding the first to the second we now have $-A_{k+1}$ and $A_k$; finally, we factor out $-1$.
We conclude: a determinant changes sign if we interchange two adjacent columns.

d) A determinant vanishes if any two of its columns are equal.
Indeed, we may bring the two columns side by side after an interchange of adjancent columns and then use axiom~\ref{ax:2}.
In the same way as in b) and c) we may now prove the more general rules:

e) Adding a multiple of one column to another does not change the value of the determinant.

f) Interchanging any two columns changes the sign of $D$.

g) Let $(\nu_1, \nu_2, \ldots, \nu_n)$ be a permutation of the subscripts $(1,2,\ldots,n)$.
If we rearrange the columns in $D(A_{\nu_1}, A_{\nu_2}, \ldots, A_{\nu_n})$ until they are back in the natural order, we see that
\[
D(A_{\nu_1}, A_{\nu_2}, \ldots, A_{\nu_n})
= \pm D(A_1, A_2, \ldots, A_n).
\]
Here $\pm$ is a definite sign that does not depend on the special values of the $A_k$.
If we substitute $U_k$ for $A_k$ w esee that $D(U_{\nu_1}, U_{\nu_2}, \ldots, U_{\nu_n}) = \pm 1$ and that the sign depends only on the permutation of the unit vectors.

Now we replace each vector $A_k$ by the following linear combination $A_k'$ of $A_1, A_2, \ldots, A_n$:
\begin{equation}
\label{eq:1F6}
A_k' = b_{1k} A_1 + b_{2k} A_2 + \cdots + b_{nk} A_n.
\end{equation}

In computing $D(A_1', A_2', \ldots, A_n')$ we first apply axiom~\ref{ax:1} on $A_1'$ breaking up the determinant into a sum; then in each term we do the same with $A_2'$ and so on.
We get
\begin{align}
\label{eq:1F7}
D(A_1', A_2', \ldots, A_n')
&= 
\!\!\!
\sum_{\nu_1,\nu_2,\ldots,\nu_n} 
\!\!\!\!
D(
        b_{\nu_1 1} A_{\nu_1},
        b_{\nu_2 2} A_{\nu_2},
        \ldots,
        b_{\nu_n n} A_{\nu_n}
)
\\
&= 
\!\!\!
\sum_{\nu_1,\nu_2,\ldots,\nu_n} 
\!\!\!\!
b_{\nu_1 1} \cdot
b_{\nu_2 2} \cdot
\cdots \cdot
b_{\nu_n n}
D(A_{\nu_1}, A_{\nu_2}, \ldots, A_{\nu_n})
\notag
\end{align}
where each $\nu_i$ runs independently from $1$ to $n$.
Should two of the indices $\nu_i$ be equal then $D(A_{\nu_1}, A_{\nu_2}, \ldots, A_{\nu_n}) = 0$; we need therefore keep only those terms in which $(\nu_1, \nu_2, \ldots, \nu_n)$ is a permutation of $(1,2,\ldots, n)$.
This gives
\begin{equation}
\label{eq:1F8}
D(A_1', A_2', \ldots, A_n')
= D(A_1, A_2, \ldots, A_n)
\!\!\!
\sum_{(\nu_1, \ldots, \nu_n)}
\!\!\!\!
\pm b_{\nu_1 1} \cdot b_{\nu_2 2} \cdot \cdots \cdot b_{\nu_n n}
\end{equation}
where $(\nu_1, \ldots, \nu_n)$ runs through all the permutations of $(1,2,\ldots,n)$ and where $\pm$ stands for the sign associated with that permutation.
It is important to remark that we would only have arrived at the same formula \eqref{eq:1F8} if our function $D$ satisfied only the first two of our axioms.

Many conclusions may be derived from \eqref{eq:1F8}.

We first assume axiom~\ref{ax:3} and specialize the $A_k$ to the unit vectors $U_k$ of \eqref{eq:1F5}.
This makes $A_k' = B_k$ where $B_k$ is the column vector of the matrix of the $b_{ik}$. \eqref{eq:1F8} yields now:
\begin{equation}
\label{eq:1F9}
D(B_1, B_2, \ldots, B_n)
= 
\!\!\!
\sum_{(\nu_1, \ldots, \nu_n)}
\!\!\!\!
\pm b_{\nu_1 1} \cdot b_{\nu_2 2} \cdot \cdots \cdot b_{\nu_n n}
\end{equation}
giving an explicit formula for determinants and showing that they are uniquely determined by our axioms provided they exist at all.

With expression \eqref{eq:1F9} we return to formula \eqref{eq:1F8} and get
\begin{equation}
\label{eq:1F10}
D(A_1', A_2', \ldots, A_n')
= 
D(A_1, A_2, \ldots, A_n)
\,
D(B_1, B_2, \ldots, B_n).
\end{equation}

This is the so-called multiplication theorem for determinants.
At the left of \eqref{eq:1F10} we have the determinant of an $n$-rowed matrix whose elements $c_{ik}$ are given by
\begin{equation}
\label{eq:1F11}
c_{ik} = \sum_{\nu=1}^n a_{1\nu} b_{\nu_k}.
\end{equation}
$c_{ik}$ is obtained by multiplying the elements of the $i$-th row of $D(A_1, A_2, \ldots, A_n)$ by those of the $k$-th column of $D(B_1, B_2, \ldots, B_n)$ and adding.

Let us now replace $D$ in \eqref{eq:1F8} by a function $F(A_1, \ldots, A_n)$ that satisfies only the first two axioms.
Comparing with \eqref{eq:1F9} we find
\[
F(A_1', A_2', \ldots, A_n')
= 
F(A_1, A_2, \ldots, A_n)
\,
D(B_1, B_2, \ldots, B_n).
\]
Specializing $A_k$ to the unit vectors $U_k$ reads to
\begin{equation}
\label{eq:1F12}
F(B_1, B_2, \ldots, B_n)
= c \cdot D(B_1, B_2, \ldots, B_n)
\end{equation}
with $c = F(U_1, U_2, \ldots, U_n)$.

Next we specialize \eqref{eq:1F10} in the following way:
If $i$ is a certain subscript from $1$ to $n-1$ we put $A_k = U_k$ for $k\not=i,i+1$, $A_i = U_i$, $A_{i+1} = 0$.
Then $D(A_1, A_2, \ldots, A_n) = 0$ since one column is $0$.
Thus, $D(A_1', A_2', \ldots, A_n') = 0$; but this determinant differs from that of the elements $b_{jk}$ only in the respect that the $(i+1)$-st row has been made equal to the $i$-th. We therefore see:

A determinant vanishes if two adjacent \emph{rows} are equal.

Each term in \eqref{eq:1F9} is a product where precisely one factor comes from a given row, say, the $i$-th.
This shows that the determinant is linear and homogeneous if considered as a function of this row.
If, finally, we select for each row the corresponding unit vector, the determinant is $= 1$ since the matrix is the same as that in which the columns are unit vectors.
This shows that a determinant satisfies our three axioms if we consider it as a function of the row vectors.
In view of the uniqueness it follows:

A determinant remains unchanged if we transpose the row vectors into column vectors, that is, if we rotate the matrix about its main diagonal.

A determinant vanishes if any two rows are equal.
It changes sign if we interchange any two rows.
It remains unchanged if we add a multiple of one row to another.


We shall now prove the existence of determinants.
For a $1$-rowed matrix $a_{11}$ the element $a_{11}$ itself is the determinant.
Let us assume the existence of $(n-1)$-rowed determinants.
If we consider the $n$-rowed matrix \eqref{eq:1F1} we may associate with it certain $(n-1)$-rowed determinants in the following way:
Let $a_{ik}$ be a particular element in \eqref{eq:1F1}.
We cancel the $i$-th row and $k$-th column in \eqref{eq:1F1} and take the determinant of the remaining $(n-1)$-rowed matrix.
The determinant multiplied by $(-1)^{i+k}$ will be called the cofactor of $a_{ik}$ and be denoted by $A_{ik}$.
The distribution of the sign $(-1)^{i+k}$ follows the chessboard pattern, namely,
\[
\begin{pmatrix}
+ & - & + & - & \cdots
\\
- & + & - & + & \cdots
\\
+ & - & + & - & \cdots
\\
- & + & - & + & \cdots
\\
\cdots & \cdots & \cdots & \cdots & \cdots
\end{pmatrix}
\]

Let $i$ be any number from $1$ to $n$.
We consider the following function $D$ of the matrix \eqref{eq:1F1}:
\begin{equation}
\label{eq:13}
D = a_{i1} A_{i1} + a_{i2} A_{i2} + \cdots + a_{in} A_{in}.
\end{equation}

It is the sum of the products of the $i$-th row and their cofactors.

Consider this $D$ in its dependence on a given column, say $A_k$.
For $\nu \not= k$, $A_{i\nu}$ depends linearly on $A_k$ and $a_\nu$ does not depend on it; for $\nu = k$, $A_{ik}$ does not depend on $A_k$ but $a_{ik}$ is one element of this column.
Thus, axiom~\ref{ax:1} is satisfied.
Assume next that two adjacent columns $A_k$ and $A_{k+1}$ are equal.
For $\nu \not= k, k+1$ we have then two equal columns in $A_{i\nu}$ so that $A_{i\nu} = 0$.
The determinants used in the computation of $A_{i,k}$ and $A_{i,k+1}$ are the same but the signs are opposite; hence, $A_{i,k} = -A_{i,k+1}$ whereas $a_{i,k} = a_{i,k+1}$.
Thus $D = 0$ and axiom~\ref{ax:2} holds.
For the special case $A_{\nu} = U_\nu$ ($\nu = 1,2,\ldots,n$) we have $a_{i\nu} = 0$ for $\nu \not= i$ while $a_{ii} = 1$.
Hence, $D = 1$ and this is axiom~\ref{ax:3}.
This proves both the existence of an $n$-rowed determinant as well as the truth of formula \eqref{eq:13}, the so-called development of a determinant according to its $i$-th row.
\eqref{eq:13} may be generalized as follows:
In our determinant replace the $i$-th row by the $j$-th row and develop according to this new row.
For $i \not= j$ that determinant is $0$ and for $i = j$ it is $D$:
\begin{equation}
\label{eq:14}
A_{j1} A_{i1} + a_{j2} A_{i2} + \cdots + a_{jn} A_{in}
= \begin{cases}
D & \text{for $j = i$,}
\\
0 & \text{for $j \not= i$.}
\end{cases}
\end{equation}
If we interchange the rows and the columns we get the following formula:
\begin{equation}
\label{eq:15}
A_{1h} A_{1k} + a_{2h} A_{2k} + \cdots + a_{nh} A_{nk}
= \begin{cases}
D & \text{for $h = k$,}
\\
0 & \text{for $h \not= k$.}
\end{cases}
\end{equation}

Now let $A$ represent an $n$-rowed and $B$ and $m$-rowed square matrix.
By $|A|$, $|B|$ we mean their determinants.
Let $C$ be a matrix of $n$ rows and $m$ columns and form the square matrix of $n+m$ rows
\begin{equation}
\label{eq:16}
\begin{pmatrix}
A & C
\\
0 & B
\end{pmatrix}
\end{equation}
where $0$ stands for a zero matrix with $m$ rows and $n$ columns.
If we consider the determinant of the matrix \eqref{eq:16} as a function of the columns of $A$ only, if it satisfies obviously the first two of our axioms.
Because of \eqref{eq:1F12} its value is $c \cdot |A|$, where $c$ is the determinant of \eqref{eq:16} after substituting unit vectors for the columns of $A$.
This $c$ still depends on $B$ and considered as a function of the rows of $B$ satisfies the first two axioms.
Therefore the determinant of \eqref{eq:16} is $d \cdot |A| \cdot |B|$ where $d$ is the special case of the determinant of \eqref{eq:16} with unit vectors for the columns of $A$ as well as of $B$.
Subtracting multiples of the columns of $A$ from $C$ we can replace $C$ by $0$.
This shows $d = 1$ and hence the formula
\begin{equation}
\label{eq:17}
\begin{vmatrix}
A & C
\\
0 & B
\end{vmatrix}
= |A| \cdot |B|.
\end{equation}

In a similar fashion we could have shows
\begin{equation}
\label{eq:18}
\begin{vmatrix}
A & 0
\\
C & B
\end{vmatrix}
= |A| \cdot |B|.
\end{equation}

The formulas \eqref{eq:17}--\eqref{eq:18} are special cases of a general theorem of Lagrange that can be derived from them.
We refer the reader to any textbook on determinants since in most application \eqref{eq:17} and \eqref{eq:18} are sufficient.

We now investigate what it means for a matrix if its determinant is zero.
We can easily establish the following facts:

a) $A_1, A_2, \ldots, A_n$ are linearly dependent, then $D(A_1, A_2, \ldots, A_n) = 0$.
Indeed one of the vectors, say $A_k$, is then a linear combination of the other columns; subtracting this linear combination from the column $A_k$ reduces it to $0$ and so $D = 0$.

b) If any vector $B$ can be expressed as a linear combination of $A_1, A_2, \ldots, A_n$, then $D(A_1, A_2, \ldots, A_n) \not= 0$.
Returning to \eqref{eq:1F6} and \eqref{eq:1F10} we may select the values for $b_{ik}$ in such a fashion that every $A_i' = U_i$.
For this choice the left side in \eqref{eq:1F10} is $1$ and hence $D(A_1, A_2, \ldots, A_n)$ on the right side $\not= 0$.

c) Let $A_1, A_2, \ldots, A_n$ be linearly independent and $B$ any other vector.
If we go back to the components in the equation $A_1 x_1 + A_2 x_2 + \cdots + A_n x_n + By = 0$ we obtain $n$ linear homogeneous equations in the $n+1$ unknowns $x_1,x_2,\ldots,x_n,y$.
Consequently, there is a non-trivial solution.
$y$ must be $\not= 0$ or else the $A_1,A_2,\ldots,A_n$ would be linearly dependent.
But then we can compute $B$ out of this equation as a linear combination of $A_1,A_2, \ldots, A_n$.

Combining these results we obtain:

A determinant vanishes if and only if the column vectors (or the row vectors) are linearly dependent.

Another way of expressing this result is:

The set of $n$ linear homogeneous equations
\[
a_{i1} x_1 + a_{i2} x_2 + \cdots + a_{in} x_n = 0
\quad
(i = 1,2,\ldots,n)
\]
in $n$ unknowns has a non-trivial solution if and only if the determinant of the coefficients is zero.

Another result that can be deduced is:

If $A_1, A_2, \ldots, A_n$ are given, then their linear combinations can represent any other vector $B$ if and only if $D(A_1,A_2, \ldots, A_n) \not= 0$.

Or:
The set of linear equations
\begin{equation}
\label{eq:19}
a_{i1} x_1 + a_{i2} x_2 + \cdots + a_{in} x_n = b_i
\quad
(i = 1,2,\ldots,n)
\end{equation}
has a solution for arbitrary values of the $b_i$ if and only if determinant of $a_{ik}$ is $\not= 0$.
In that case the solution is unique.

We finally express the solution of \eqref{eq:19} by means of determinants if the determinant $D$ of the $a_{ik}$ is $\not= 0$.

We multiply for a given $k$ the $i$-th equation with $A_{ik}$ and add the equations.
\eqref{eq:15} gives
\begin{equation}
\label{eq:20}
D \cdot x_k = A_{1k} b_1 + A_{2k} b_k + \cdots + A_{nk} b_n
\quad
(k = 1,2,\ldots,n)
\end{equation}
and this gives $x_k$.
The right side in \eqref{eq:1F12} may also be written as the determinant obtained from $D$ by replacing the $k$-th column by $b_1,b_2,\ldots,b_n$.
The rule thus obtained is known as Cramer's rule.




\section{Field theory}


\subsection{Extension fields}

If $E$ is a field and $F$ a subset of $E$ which, under the operations of addition and multiplication in $E$, itself forms a field, that is, if $F$ is a subfield of $E$, then we shall call $E$ an \emph{extension} of $F$.
The relation of being an extension of $F$ will be briefly designated by $F \subset E$.
If $\alpha, \beta, \gamma, \ldots$ are elements of $E$, then by $F(\alpha, \beta, \gamma, \ldots)$ we shall mean the set of elements in $E$ which can be expressed as quotients of polynomials in $\alpha, \beta, \gamma, \ldots$ with coefficients in $F$.
It is clear that $F(\alpha, \beta, \gamma, \ldots)$ is a field and is the smallest extension of $F$ which contains the elements $\alpha, \beta, \gamma, \ldots$.
We shall call $F(\alpha, \beta, \gamma, \ldots)$ the field obtained after the \emph{adjunction} of the elements $\alpha, \beta, \gamma, \ldots$ to $F$, or the field \emph{generated} out of $F$ by the elements $\alpha, \beta, \gamma, \ldots$.
In the sequel all fields will be assumed commutative.

If $F \subset E$, then ignoring the operation of multiplication defined between the elements of $E$, we may consider $E$ as a vector space over $F$.
By the \emph{degree} of $E$ over $F$, written $(E/F)$, we shall mean the dimension of the vector space $E$ over $F$.
If $(E/F)$ is finite, $E$ will be called a \emph{finite extension}.


\begin{theo}
\label{theo:si}
If $F, B, E$ are three fields such that $F \subset B \subset E$, then
\[
(E/F) = (B/F) \, (E/B).
\]
\end{theo}


Let $A_1, A_2, \ldots, A_r$ be elements of $E$ which are linearly independent with respect to $B$ and let $C_1, C_2, \ldots, C_s$ be elements of $B$ which are independent with respect to $F$.
Then the products $C_i A_j$ where $i = 1,2,\ldots, s$ and $j=1,2,\ldots,r$ are elements of $E$ which are independent with respect to $F$.
For if $\sum_{i,j} a_{ij} C_i A_j = 0$, then
$\sum_j (\sum_i a_{ij} C_i) A_j$ is a linear combination of the $A_j$ with coefficients in $B$ and because the $A_j$ were independent with respect to $B$ we have $\sum_i a_{ij} C_i = 0$ for each $j$.
The independence of $C_i$ with respect to $F$ then requires that each $a_{ij} = 0$.
Since there are $r \cdot s$ elements $C_i A_j$ we have shown that for each $r \leq (E/B)$ and $s \leq (B/F)$ the degree $(E/F) \geq r \cdot s$.
Therefore, $(E/F) \leq (B/F) (E/B)$.
If one of the latter numbers is infinite, the theorem follows.
If both $(E/B)$ and $(B/F)$ are finite, say $r$ and $s$ respectively, we may suppose that the $A_j$ and the $C_i$ are generating systems of $E$ an $B$ respectively, and we show that the set of products $C_i A_j$ is a generating system of $E$ over $F$.
Each $A \in E$ can be expressed linearly in terms of the $A_j$ with coefficients in $B$.
Thus, $A = \sum B_j A_j$
Moreover, each $B_j$ being an element of $B$ can be expressed linearly with coefficients in $F$ in terms of the $C_i$, i.e., $B_j = \sum a_{ij} C_i$, $j=1,2,\ldots,r$.
Thus, $A = \sum a_{ij} C_i A_j$ and the $C_iA_j$ for an independent generating system of $E$ over $F$.


\begin{coro*}
\label{coro:p22}
If $F \subset F_1 \subset F_2 \subset \cdots \subset F_n$, then 
\[
(F_n/F) = (F_1/F) \cdot (F_2/F_1) \, \cdots \, (F_n/F_{n-1}).
\]
\end{coro*}


\subsection{Polynomials}

An expression of the form $a_0 x^n + a_1 x^{n-1} + \cdots + a_n$ is called a \emph{polynomial} in $F$ of degree $n$ if the coefficients $a_0, \ldots, a_n$ are elements of the field $F$ and $a_0 \not= 0$.
Multiplication and addition of polynomials are performed in the usual way.%
\footnote{If we speak of the set of all polynomials of degree lower than $n$, we shall agree to include the polynomial $0$ in this set, though it has no degree in the proper sense.}

A polynomial in $F$ is called \emph{reducible} in $F$ if it is equal to the product of two polynomials in $F$ each of degree at least one.
Polynomials which are not reducible in $F$ are called \emph{irreducible} in $F$.

If $f(x) = g(x) \cdot h(x)$ is a relation which holds between polynomials $f(x)$, $g(x)$, $h(x)$ in a field $F$, then we shall say that $g(x)$ \emph{divides} $f(x)$ in $F$, or that $g(x)$ is a \emph{factor} of $f(x)$.
It is readily seen that the degree of $f(x)$ is equal to the sum of the degrees of $g(x)$ and $h(x)$, so that if neither $g(x)$ nor $h(x)$ is a constant then each has a degree less than $f(x)$.
It follows from this that by a finite number of factorizations a polynomial can always be expressed as a product of irreducible polynomials in a field $F$.

For any two polynomials $f(x)$ and $g(x)$ the division algorithm holds, i.e., $f(x) = q(x) g(x) + r(x)$ where $q(x)$ and $r(x)$ are unique polynomials in $F$ and the degree of $r(x)$ is less than that of $g(x)$.
This may be shown by the argument as the reader met in elementary algebra in the case of the field of real or complex numbers.
We also see that $r(x)$ is the uniquely determined polynomial of a degree less than that of $g(x)$ such that $f(x) - r(x)$ is divisible by $g(x)$.
We shall call $r(x)$ the \emph{remainder} of $f(x)$.

Also, in the usual way, it may be shown that if $\alpha$ is a root of the polynomial $f(x)$ in $F$ then $x - \alpha$ is a factor of $f(x)$, and as a consequence of this that a polynomial in a field cannot have more roots in the field than its degree.


\begin{lemm*}
\label{lemm:p24}
If $f(x)$ is an irreducible polynomial of degree $n$ in $F$, then there do not exist two polynomials each of degree less than $n$ in $F$ whose product is divisible by $f(x)$.
\end{lemm*}

Let us suppose to the contrary that $g(x)$ and $h(x)$ are polynomials of degree less than $n$ whose product is divisible by $f(x)$.
Among all polynomials occurring in such pairs we may suppose $g(x)$ has the smallest degree.
Then since $f(x)$ is a factor of $g(x) \cdot h(x)$ there is a polynomial $k(x)$ such that
\[
k(x) \cdot f(x) = g(x) \cdot h(x).
\]
By the division algorithm,
\[
f(x) = q(x) \cdot g(x) + r(x)
\]
where the degree of $r(x)$ is less than that of $g(x)$ and $r(x) \not= 0$ since $f(x)$ was assumed to be irreducible.
Multiplying
\[
f(x) = q(x) \cdot g(x) + r(x)
\]
by $h(x)$ and transposing, we have
\[
r(x) \cdot h(x)
= f(x) \cdot h(x) - q(x) \cdot g(x) \cdot h(x)
= f(x) \cdot h(x) - q(x) \cdot k(x) \cdot f(x)
\]
from which it follows that $r(x) \cdot h(x)$ is divisible by $f(x)$.
Since $r(x)$ has a smaller degree than $g(x)$, this last is in contradiction to the choice of $g(x)$, from which the lemma follows.

As we saw, many of the theorems of elementary algebra hold in any field~$F$.
However, the so-called Fundamental Theorem of Algebra, at least in its customary form, does not hold.
It will be replaced by a theorem due to Kronecker which guarantees for a given polynomial in $F$ the existence of an extension field in which the polynomial has a root.
We shall also show that, in a given field, a polynomial can not only be factored into irreducible factors, but that this factorization is unique up to a constant factor.
The uniqueness depends on the theorem of Kronecker.


\subsection{Algebraic elements}

Let $F$ be a field and $E$ an extension field of $F$.
If $\alpha$ is an element of $E$ we may ask whether there are polynomials with coefficients in $F$ which have $\alpha$ as a root.
$\alpha$ is called \emph{algebraic} with respect to $F$ if there are such polynomials.
Now let $\alpha$ be algebraic and select among all polynomials in $F$ which have $\alpha$ as root one, $f(x)$, of lowest degree.

We may assume that the highest coefficient of $f(x)$ is $1$.
We content that this $f(x)$ is uniquely determined, that it is irreducible and that each polynomial in $F$ with the root $\alpha$ is divisible by $f(x)$.
If, indeed, $g(x)$ is a polynomial in $F$ with $g(\alpha) = 0$, we may divide $g(x) = f(x) q(x) + r(x)$ where $r(x)$ has a degree smaller than that of $f(x)$.
\label{p25}%
Substituting $x = \alpha$ we get $r(\alpha) = 0$.
Now $r(x)$ has to be identically $0$ since otherwise $r(x)$ would have the root $\alpha$ and be of degree lower than $f(x)$.
So $g(x)$ is divisible by $f(x)$.
This also shows the uniqueness of $f(x)$.
If $f(x)$ were not irreducible, one of the factors would have to vanish for $x = \alpha$ contradicting again the choice of $f(x)$.

We now consider the subset $E_0$ of the following elements $\theta$ of $E$:
\[
\theta = g(\alpha) = c_0 + c_1\alpha + c_2\alpha^2 + \cdots + c_{n-1} \alpha^{n-1}
\]
where $g(x)$ is a polynomial in $F$ of degree less than $n$ ($n$ being the degree of $f(x)$).
This set $E_0$ is closed under addition and multiplication.
The latter may be verified as follows:

If $g(x)$ and $h(x)$ are two polynomials of degree less than $n$ we put 
\[
g(x) h(x) = q(x) f(x) + r(x)
\]
and hence $g(\alpha) h(\alpha) = r(\alpha)$.
Finally we see that the constants $c_0, c_1, \ldots, c_{n-1}$ are uniquely determined by the element $\theta$.
Indeed two expressions for the same $\theta$ would lead after subtracting to an equation for $\alpha$ of lower degree than $n$.

We remark that the internal structure of the set $E_0$ does not depend on the nature of $\alpha$ but only on the irreducible $f(x)$.
The knowledge of this polynomial enables us to perform the operations of addition and multiplication in our set $E_0$.
We shall see very soon that $E_0$ is a field; in fact, $E_0$ is nothing but the field $F(\alpha)$.
As soon as this is shown we have at once the degree, $(F(\alpha)/F)$, determined as $n$, since the space $F(\alpha)$ is generated by the linearly independent $1, \alpha, \alpha^2, \ldots, \alpha^{n-1}$.

We shall now try to imitate the set $E_0$ without having an extension field $E$ and an element $\alpha$ at our disposal.
We shall assume only an irreducible polynomial
\[
f(x) = x^n + a_{n-1} x^{n-1} + \cdots + a_0
\]
as given.

We select a symbol $\xi$ and let $E_1$ be the set of all formal polynomials
\[
g(\xi) = c_0 + c_1 \xi + \cdots + c_{n-1} \xi^{n-1}
\]
of a degree lower than $n$.
This set forms a group under addition.
We now introduce besides the ordinary multiplication a new kind of multiplication of two elements $g(\xi)$ and $h(\xi)$ of $E_1$ denoted by $g(\xi) \times h(\xi)$.
It is defined as the remainder $r(\xi)$ of the ordinary product $g(\xi) h(\xi)$ under division by $f(\xi)$.
We first remark that any product of $m$ terms $g_1(\xi), g_2(\xi), \ldots, g_m(\xi)$ is again the remainder of the ordinary product $g_1(\xi) g_2(\xi) \cdots g_m(\xi)$.
This is true by definition for $m = 2$ and follows for every $m$ by induction if we just prove the easy lemma: The remainder of the product of two remainders (of two polynomials) is the remainder of the product of these two polynomials.
This fact shows that our new product is associative and commutative and also that the new product $g_1(\xi) \times g_2(\xi) \times \cdots \times g_m(\xi)$ will coincide with the old product $g_1(\xi) g_2(\xi) \cdots g_m(\xi)$ if the latter does not exceed $n$ in degree.
The distributive law for our multiplication is readily verified.

The set $E_1$ contains our field $F$ and our multiplication in $E_1$ has for $F$ the meaning of the old multiplication.
One of the polynomials of $E_1$ is $\xi$.
Multiplying it $i$-times with itself, clearly will just lead to $\xi^i$ as long as $i < n$.
For $i = n$ this is not any more the case since it leads to the remainder of the polynomial $\xi^n$.
This remainder is
\[
\xi^n - f(\xi)
= -a_{n-1} \xi^{n-1} - a_{n-2} \xi^{n-2} - \cdots - a_0.
\]

We now give up our old multiplication altogether and keep only the new one;
we also change notation, using the point (or juxtaposition) as symbol for the new multiplication.

Computing in this sense
\[
c_0 + c_1 \xi + c_2 \xi^2 + \cdots + c_{n-1} \xi^{n-1}
\]
will readily lead to this element, since all the degrees involved are below $n$.
But
\[
\xi^n
= -a_{n-1} \xi^{n-1} - a_{n-2} \xi^{n-2} - \cdots - a_0.
\]
Transposing we see that $f(\xi) = 0$.

We have thus constructed a set $E_1$ and an addition and multiplication in $E_1$ that already satisfies most of the field axioms.
$E_1$ contains $F$ as a subfield an $\xi$ satisfies the equation $f(\xi) = 0$.
We next have to show:
If $g(\xi) \not= 0$ and $h(\xi)$ are given elements of $E_1$, there is an element
\[
X(\xi) = x_0 + x_1 \xi + \cdots + x_{n-1} \xi^{n-1}
\]
in $E_1$ such that
\[
g(\xi) \cdot X(\xi) = h(\xi).
\]
To prove it we consider the coefficients $x_i$ of $X(\xi)$ as unknowns and compute nevertheless the product on the left side, always reducing higher powers of $\xi$ to lower ones.
The result is an expression $L_0 + L_1 \xi + \cdots + L_{n-1} \xi^{n-1}$ where each $L_i$ is a linear combination of the $x_i$ with coefficients in $F$.
This expression is to be equal to $h(\xi)$; this leads to the $n$ equations with $n$ unknowns:
\[
L_0 = b_0,
\quad
L_1 = b_1,
\quad
\ldots, \quad
L_{n-1} = b_{n-1}
\]
where the $b_i$ are the coefficients of $h(\xi)$.
This system will be soluble if the corresponding homogeneous equations
\[
L_0 = 0,
\quad
L_1 = 0,
\quad
\ldots, \quad
L_{n-1} = 0
\]
have only the trivial solution.

The homogeneous problem would occur if we should ask for the set of elements $X(\xi)$ satisfying $g(\xi) \cdot X(\xi) = 0$.
Going back for a moment to the old multiplication this would mean that the ordinary product $g(x) X(\xi)$ has the remainder $0$, and is therefore divisible by $f(\xi)$.
According to the lemma, page~\pageref{lemm:p24}, this is only possible for $X(\xi) = 0$.

Therefore $E_1$ is a field.

Assume now that we also have our old extension $E$ with a root $\alpha$ of $f(x)$, leading to the set $E_0$.
We see that $E_0$ has in a certain sense the same structure as $E_1$ if we map the element $g(\xi)$ of $E_1$ onto the element $g(\alpha)$ of $E_0$.
This mapping will have the property that the image of a sum of elements is the sum of the images, and the image of a product is the product of the images.

Let us therefore define:
A mapping $\sigma$ of one field onto another which is one to one in both directions such that $\sigma(\alpha + \beta) = \sigma(\alpha) + \sigma(\beta)$ and $\sigma(\alpha \cdot \beta) = \sigma(\alpha) \cdot \sigma(\beta)$ is called an \emph{isomorphism}.
If the fields in question are not distinct -- i.e., are both the same field -- the isomorphism is called an \emph{automorphism}.
Two fields for which there exists an isomorphism mapping one on another are called \emph{isomorphic}.
If not every element of the image field is the image under $\sigma$ of an element in the first field, then $\sigma$ is called an isomorphism of the first field \emph{into} the second.
Under each isomorphism it is clear that $\sigma(0) = 0$ and $\sigma(1) = 1$.

We see that $E_0$ is also a field and that it is isomorphic to $E_1$.

We now mention a few theorems that follow from our discussion:

\begin{theo}[Kronecker]
\label{theo:se}
If $f(x)$ is a polynomial in a field $F$, there exists an extension $E$ of $F$ in which $f(x)$ has a root.
\end{theo}

\begin{proof}
Construct an extension field in which an irreducible factor of $f(x)$ has a root.
\end{proof}

\begin{theo}
\label{theo:ei}
Let $\sigma$ be an isomorphism mapping a field $F$ on a field $F'$.
Let $f(x)$ be an irreducible polynomial in $F$ and $f'(x)$ the corresponding polynomial in $F'$.
If $E = F(\beta)$ and $E' = F'(\beta')$ are extensions of $F$ and $F'$, respectively, where $f(\beta) = 0$ in $E$ and $f'(\beta') = 0$ in $E'$, then $\sigma$ can be extended to an isomorphism between $E$ and $E'$.
\end{theo}

\begin{proof}
$E$ and $E'$ are both isomorphic to $E_0$.
\end{proof}


\subsection{Splitting fields}

If $F$, $B$ and $E$ are three fields such that $F \subset B \subset E$, then we shall refer to $B$ as an \emph{intermediate field}.

If $E$ is an extension of a field $F$ in which a polynomial $p(x)$ in $F$ can be factored into linear factors, and if $p(x)$ can not so be factored in any intermediate field, then we call $E$ a \emph{splitting} field for $p(x)$.
Thus, if $E$ is a splitting field of $p(x)$, \emph{the roots of $p(x)$ generate $E$}.

A splitting field is of finite degree since it is constructed by a finite number of adjunctions of algebraic elements, each defining an extension field of finite degree.
Because of the corollary on page~\pageref{coro:p22}, the total degree is finite.

\begin{theo}
\label{theo:ni}
If $p(x)$ is a polynomial in a field $F$, there exists a splitting field $E$ of $p(x)$.
\end{theo}

We factor $p(x)$ in $F$ into irreducible factors $f_1(x) \cdots f_r(x) = p(x)$.
If each of these is of the first degree then $F$ itself is the required splitting field.
Suppose then that $f_1(x)$ is of degree higher than the first.
By Theorem~\ref{theo:se} there is an extension $F_1$ of $F$ in which $f_1(x)$ has a root.
Factor each of the factors $f_1(x), \ldots, f_r(x)$ into irreducible factors in $F_1$ and proceed as before.
We finally arrive at a field in which $p(x)$ can be split into linear factors.
The field generated out of $F$ by the roots of $p(x)$ is the required splitting field.

The following theorem asserts that up to isomorphisms, the splitting field of a polynomial is unique.

\begin{theo}
\label{theo:onze}
Let $\sigma$ be an isomorphism mapping the field $F$ on the field $F'$.
Let $p(x)$ be a polynomial in $F$ and $p'(x)$ the polynomial in $F'$ with coefficients corresponding to those of $p(x)$ under $\sigma$.
Finally, let $E$ be a splitting field of $p(x)$ and $E'$ a splitting field of $p'(x)$.
Under these conditions the isomorphism $\sigma$ can be extended to an isomorphism between $E$ and $E'$.
\end{theo}

If $f(x)$ is an irreducible factor of $p(x)$ in $F$, then $E$ contains a root of $f(x)$.
For let $p(x) = (x - \alpha_1) (x - \alpha_2) \cdots (x - \alpha_s)$ be the splitting of $p(x)$ in $E$.
Then $(x - \alpha_1) (x - \alpha_2) \cdots (x - \alpha_s) = f(x) g(x)$.
We consider $(x)$ as a polynomial in $E$ and construct the extension field $B = E(\alpha)$ in which $f(\alpha) = 0$.
Then $(\alpha - \alpha_1) \cdot (\alpha - \alpha_2) \cdot \cdots \cdot (\alpha - \alpha_s) = f(\alpha) \cdot g(\alpha) = 0$ and $\alpha - \alpha_i$ being elements of the field $B$ can have a product equal to $0$ only if for one of the factors, say the first, we have $\alpha - \alpha_1 = 0$.
Thus, $\alpha = \alpha_1$, and $\alpha_1$ is a root of $f(x)$.

Now in case all roots of $p(x)$ are in $F$, then $E = F$ and $p(x)$ can be split in $F$.
This factored form has an image in $F'$ which is a splitting of $p'(x)$, since the isomorphism $\sigma$ preserves all operations of addition and multiplication in the process of multiplying out the factors of $p(x)$ and collecting to get the original form.
Since $p'(x)$ can be split in $F'$, we must have $F' = E'$.
In this case, $\sigma$ itself is the required extension and the theorem is proved if all roots of $p(x)$ are in $F$.

We proceed by complete induction.
Let us suppose the theorem proved for all cases in which the number of roots of $p(x)$ outside of $F$ is less than $n > 1$, and suppose that $p(x)$ is a polynomial having $n$ roots outside of $F$.
We factor $p(x)$ into irreducible factors in $F$; $p(x) = f_1(x) f_2(x) \cdots f_m(x)$.
Not all of these factors can be of degree 1, since otherwise $p(x)$ would split in $F$, contrary to assumption.
Hence, we may suppose the degree of $f_1(x)$ to be $r > 1$.
Let $f'_1(x) f'_2(x) \cdots f'_m(x) = p'(x)$ be the factorization of $p'(x)$ into the polynomials corresponding to $f_1(x), \ldots, f_m(x)$ under $\sigma$.
$f'_1(x)$ is irreducible in $F'$, for a factorization of $f'_1(x)$ in $F'$ would induce\footnote{See page~\pageref{p38} for the definition of $\sigma^{-1}$.} under $\sigma^{-1}$ a factorization of $f_1(x)$, which was however taken to be irreducible.

By Theorem~\ref{theo:ei}, the isomorphism $\sigma$ can be extended to an isomorphism $\sigma_1$, between the fields $F(\alpha)$ and $F'(\alpha')$.

Since $F \subset F(\alpha)$, $p(x)$ is a polynomial in $F(\alpha)$ and $E$ is a splitting field for $p(x)$ in $F(\alpha)$.
Similarly for $p'(x)$.
There are now less than $n$ roots of $p(x)$ outside the new ground field $F(\alpha)$.
Hence by our inductive assumption $\sigma_1$ can be extended from an isomorphism between $F(\alpha)$ and $F'(\alpha')$ to an isomorphism $\sigma_2$ between $E$ and $E'$.
Since $\sigma_1$ is an extension of $\sigma$, and $\sigma_2$ and extension of $\sigma_1$, we conclude $\sigma_2$ is an extension of $\sigma$ and the theorem follows.


\begin{coro*}
If $p(x)$ is a polynomial in a field $F$, then any two splitting fields for $p(x)$ are isomorphic.
\end{coro*}

This follows from Theorem~\ref{theo:onze} if we take $F = F'$ and $\sigma$ to be the identity mapping, i.e., $\sigma(x) = x$.

As a consequence of this corollary we see that we are justified in using the expression ``the splitting field of $p(x)$'' since any two differ only by an isomorphism.
Thus, if $p(x)$ has repeated roots in one splitting field, so also in any other splitting field it will have repeated roots.
The statement ``$p(x)$ has repeated roots'' will be significant without reference to a particular splitting field.



\subsection{Unique decomposition of polynomials into irreducible factors}


\begin{theo}
\label{theo:onon}
If $p(x)$ is a polynomial in a field $F$, and if $p(x) = p_1(x) \cdot p_2(x) \cdot \cdots \cdot p_r(x) = q_1(x) \cdot q_2(x) \cdot \cdots \cdot q_s(x)$ are two factorizations of $p(x)$ into irreducible polynomials each of degree at least one, then $r = s$ and after a suitable change in the order in which the $q$'s are written, $p_i(x) = c_i q_i(x)$, $i = 1,2,\ldots,r$ and $c_i \in F$.
\end{theo}

Let $F(\alpha)$ be an extension of $F$ in which $p_1(\alpha) = 0$.
We may suppose the leading coefficients of the $p_i(x)$ and the $q_i(x)$ to be 1, for, by factoring out all leading coefficients and combining, the constant multiplier on each side of the equation must be the leading coefficient of $p(x)$ and hence can be divided out of both sides of the equation.
Since $0 = p_1(\alpha) \cdot p_2(\alpha) \cdot \cdots \cdot p_r(\alpha) = p(\alpha) = q_1(\alpha) \cdot \cdots \cdot q_s(\alpha)$ and since a product of elements of $F(\alpha)$ can be 0 only if one of these is 0, it follows that one of the $q_i(\alpha)$, say $q_1(\alpha)$ is 0.
This gives (see page~\pageref{p25}) $p_1(x) = q_1(x)$.
Thus $p_1(x) \cdot p_2(x) \cdot \cdots \cdot p_r(x) = p_1(x) \cdot q_2(x) \cdot \cdots \cdot q_s(x)$ or $p_1(x) \cdot (p_2(x) \cdot \cdots \cdot p_r(x) - q_2(x) \cdot \cdots \cdot q_s(x)) = 0$.
Since the product of two polynomials is $0$ only if one of the two is the $0$ polynomial, it follows that the polynomial within the brackets is 0 so that $p_2(x) \cdot \cdots \cdot p_r(x) = q_2(x) \cdot \cdots \cdot q_s(x)$.
If we repeat the above argument $r$ times we obtain $p_i(x) = q_i(x)$, $i = 1,2,\ldots, r$.
Since the remaining $q$'s must have a product 1, it follows that $r = s$.


\subsection{Group characters}

If $G$ is a multiplicative group, $F$ a field and $\sigma$ a homomorphism mapping $G$ into $F$, then $\sigma$ is called a \emph{character} of $G$ in $F$.
By homomorphism is meant a mapping $\sigma$ such that for $\alpha, \beta$ any two elements of $G$, $\sigma(\alpha) \cdot \sigma(\beta) = \sigma(\alpha \cdot \beta)$ and $\sigma(\alpha) \not= 0$ for any $\alpha$.
(If $\sigma(\alpha) = 0$ for one element $\alpha$, then $\sigma(x) = 0$ for each $x \in G$, since $\sigma(\alpha y) = \sigma(\alpha) \sigma(y) = 0$ and $\alpha y$ takes all values in $G$ when $y$ assumes all values in $G$.)

The characters $\sigma_1, \sigma_2, \ldots, \sigma_n$ are called \emph{dependent} if there exist elements $a_1, a_2, \ldots a_n$ not all zero in $F$ such that $a_1 \sigma_1(x) + a_2 \sigma_2(x) + \cdots + a_n \sigma_n(x) = 0$ for each $x \in G$.
Such a dependence relation is called \emph{non-trivial}.
If the characters are not dependent they are called \emph{independent}.


\begin{theo}
\label{theo:ontw}
If $G$ is a group and $\sigma_1, \sigma_2, \ldots, \sigma_n$ are $n$ mutually distinct characters of $G$ in a field $F$, the $\sigma_1, \sigma_2, \ldots, \sigma_n$ are independent.
\end{theo}


One character cannot be dependent, since $a_1 \sigma_1(x) = 0$ implies $a_1 = 0$ due to the assumption that $\sigma_1(x) \not= 0$.
Suppose $n > 1$.
We make the inductive assumption that no set of less than $n$ distinct characters is dependent.
Suppose now that $a_1 \sigma_1(x) + a_2 \sigma_2(x) + \cdots + a_n \sigma_n(x) = 0$ is a non-trivial dependence between the $\sigma$'s.
None of the elements $a_i$ is zero, else we should have a dependence between less than $n$ characters contrary to our inductive assumption.
Since $\sigma_1$ and $\sigma_n$ are distinct, there exists an element $\alpha$ in $G$ such that $\sigma_1(\alpha) \not= \sigma_n(\alpha)$.
Multiply the relation between the $\sigma$'s by $a_n^{-1}$.
We obtain a relation
\begin{equation}
\label{eq:star}
b_1 \sigma_1(x) + \cdots + b_{n-1} \sigma_{n-1}(x) + \sigma_n(x) = 0,
\quad
b_i = a_n^{-1} a_i \not= 0.
\tag{$*$}
\end{equation}
Replace in this relation $x$ by $\alpha x$. We have
\[
b_1 \sigma_1(\alpha) \sigma_1(x) 
+ \cdots 
+ b_{n-1} \sigma_{n-1} (\alpha) \sigma_{n-1}(x) 
+ \sigma_n(\alpha) \sigma_n(x) = 0,
\]
or $\sigma_n^{-1} b_1 \sigma_1(\alpha) \sigma_1(x) + \cdots + \sigma_n(x) = 0$.
Subtracting the latter from \eqref{eq:star} we have
\begin{equation}
\label{eq:starstar}
\bigl(b_1 - \sigma_n(\alpha)^{-1}b_1 \sigma_1(\alpha)\bigr) \sigma_1(x)
+ \cdots
+ c_{n-1} \sigma_{n-1}() = 0.
\tag{$**$}
\end{equation}
The coefficient of $\sigma_1(x)$ in this relation is not 0, otherwise we should have $b_1 = \sigma_n(\alpha)^{-1}b_1\sigma_1(\alpha)$, so that
\[
\sigma_n(\alpha) b_1 = b_1 \sigma_1(\alpha) = \sigma_1(\alpha)b_1
\]
and since $b_1 \not= 0$, we get $\sigma_n(\alpha) = \sigma_1(\alpha)$ contrary to the choice of $\alpha$.
Thus, \eqref{eq:starstar} is a non-trivial dependence between $\sigma_1, \sigma_2, \ldots, \sigma_{n-1}$ which is contrary to our inductive assumption.


\begin{coro*}
If $E$ and $E'$ are two fields, and $\sigma_1, \sigma_2, \ldots, \sigma_n$ are $n$ mutually distinct isomorphisms mapping $E$ into $E'$, then $\sigma_1, \ldots, \sigma_n$ are independent.
(Where ``independent'' again means that there exists no non-trivial dependence $a_1 \sigma_1(x) + \cdots + a_n \sigma_n(x) = 0$ which holds for every $x \in E$.)
\end{coro*}


This follows from Theorem~\ref{theo:ontw}, since $E$ without the 0 is a group and the $\sigma$'s defined in this group are mutually distinct characters.

If $\sigma_1, \sigma_2, \ldots, \sigma_n$ are isomorphisms of a field $E$ into a field $E'$, then each element $a$ of $E$ such that $\sigma_1(a) = \sigma_2(a) = \ldots = \sigma_n(a)$ is called a \emph{fixed point} of $E$ under $\sigma_1, \sigma_2, \ldots, \sigma_n$.
This name is chosen because in the case where the $\sigma$'s are automorphisms and $\sigma_1$ is the identity, i.e., $\sigma_1(x) = x$, we have $\sigma_i(x) = x$ for a fixed point.


\begin{lemm*}
The set of fixed points of $E$ is a subfield of $E$.
We shall call this subfield the fixed field.
\end{lemm*}

For if $a$ and $b$ are fixed points, then $\sigma_i(a + b) = \sigma_i(a) + \sigma_i(b) = \sigma_j(a) + \sigma_j(b) = \sigma_j(a + b)$ and $\sigma_i(a \cdot b) = \sigma_i(a) \cdot \sigma_i(b) = \sigma_j(a) \cdot \sigma_j(b) = \sigma_j(a \cdot b)$.
Finally from $\sigma_i(a) = \sigma_j(a)$ we have $(\sigma_j(a))^{-1} = (\sigma_i(a))^{-1} = \sigma_i(a^{-1}) = \sigma_j(a^{-1})$.
Thus, the sum and products of two fixed points is a fixed point, and the inverse of a fixed point is a fixed point.
Clearly, the negative of a fixed point is a fixed point.


\begin{theo}
\label{theo:onth}
If $\sigma_1, \ldots, \sigma_n$ are $n$ mutually distinct isomorphisms of a field $E$ into a field $E'$, and if $F$ is the fixed field of $E$, then $(E/F) \geq n$.
\end{theo}

Suppose to the contrary that $(E/F) = r < n$.
We shall show that we are led to a contradiction.
Let $\omega_1, \omega_2, \ldots, \omega_r$ be a generating system of $E$ of $F$.
In the homogeneous linear equations
\begin{align*}
\sigma_1(\omega_1)x_1 + \sigma_2(\omega_1)x_2 + \cdots + \sigma_n(\omega_1)x_n &= 0
\\
\sigma_1(\omega_2)x_1 + \sigma_2(\omega_2)x_2 + \cdots + \sigma_n(\omega_2)x_n &= 0
\\
\vdots &
\\
\sigma_1(\omega_r)x_1 + \sigma_2(\omega_r)x_2 + \cdots + \sigma_n(\omega_r)x_n &= 0
\end{align*}
there are more unknowns than equations so that there exists a non-trivial solution which, we may suppose, $x_1, x_2, \ldots, x_n$ denotes.
For any element $\alpha$ in $E$ we can find $a_1, a_2, \ldots, a_r$ in $F$ such that $\alpha = a_1 \omega_1 + \cdots + a_r \omega_r$.
We multiply the first equation by $\sigma_1(a)$, the second by $\sigma_1(a_2)$, and so on.
Using that $a_i \in F$, hence that $\sigma_1(a_i) = \sigma_j(a_i)$ and also that $\sigma_j(a_i) \sigma_j(\omega_i) = \sigma_j(a_i \omega_i)$, we obtain
\begin{align*}
\sigma_1(a_1 \omega_1)x_1 + \cdots + \sigma_n(a_1 \omega_1)x_n &= 0
\\
\vdots &
\\
\sigma_1(a_r \omega_r)x_1 + \cdots + \sigma_n(a_r \omega_r)x_n &= 0.
\end{align*}
Adding these last equations and using
\[
\sigma_i(a_1 \omega_1) + \sigma_i(a_2 \omega_2) + \cdots + \sigma_i(a_r \omega_r)
= \sigma_i(a_1 \omega_1 + \cdots + a_r \omega_r) 
= \sigma_i(\alpha)
\]
we obtain
\[
\sigma_1(\alpha)x_1 + \sigma_2(\alpha)x_2 + \cdots + \sigma_n(\alpha)x_n = 0.
\]
This, however, is a non-trivial dependence relation between $\sigma_1, \sigma_2, \ldots, \sigma_n$ which cannot exist according to the corollary of Theorem~\ref{theo:ontw}. 


\begin{coro*}
If $\sigma_1, \sigma_2, \ldots, \sigma_n$ are automorphisms of the field $E$, and $F$ is the fixed field, then $(E/F) \geq n$.
\end{coro*}


If $F$ is a subfield of the field $E$, and $\sigma$ an automorphism of $E$, we shall say that $\sigma$ \emph{leaves $F$ fixed} if for each element $a$ of $F$, $\sigma(a) = a$.
If $\sigma$ and $\tau$ are two automorphisms of $E$, then the mapping $\sigma(\tau(x))$ written briefly $\sigma\tau$ is an automorphism, as the reader may readily verify.
(E.G., $\sigma\tau(x\cdot y) = \sigma(\tau(x \cdot y)) = \sigma(\tau(x) \cdot \tau(y)) = \sigma(\tau(x)) \cdot \sigma(\tau(y))$.)
We shall call $\sigma\tau$ the \emph{product} of $\sigma$ and $\tau$.
If $\sigma$ is an automorphism ($\sigma(x) = y$), then we shall call $\sigma^{-1}$ the mapping of $y$ into $x$, i.e., $\sigma^{-1}(y) = x$ the inverse of $\sigma$.\label{p38}
The reader may readily verify that $\sigma^{-1}$ is an automorphism.
The automorphism $I(x) = x$ shall be called the \emph{unit automorphism}.


\begin{lemm*}
If $E$ is an extension field of $F$, the set $G$ of automorphisms which leave $F$ fixed is a group.
\end{lemm*}


The product of two automorphisms which leave $F$ fixed clearly leaves $F$ fixed.
Also, the inverse of any automorphism in $G$ is in $G$.

The reader will observe that $G$, the set of automorphisms which leave $F$ fixed, does not necessarily have $F$ as its fixed field.
It may be that certain elements in $E$ which do not belong to $F$ are left fixed by every automorphism which leaves $F$ fixed.
Thus, the fixed field of $G$ may be larger than $F$.


\subsection[Applications and examples]{Applications and examples to Theorem~\ref{theo:onth}}


Theorem \ref{theo:onth} is very powerful as the following examples show:


1)
Let $k$ be a field and consider the field $E = k(x)$ of all rational functions of the variable $x$.
If we map each of the functions $f(x)$ of $E$ onto $f(1/x)$ we obviously obtain an automorphism of $E$.
Let us consider the following six automorphisms where $f(x)$ is mapped onto $f(x)$ (identity), $f(1-x)$, $f(1/x)$, $f(1-1/x)$, $f(1/(1-x))$ and $f(x/(x-1))$ and call $F$ the fixed point field.
$F$ consists of all rational functions satisfying
\begin{equation}
\label{eq:twGon}
f(x)
= f(1-x)
= f\Bigl(\frac 1x\Bigr)
= f\Bigl(1 - \frac 1x\Bigr)
= f\Bigl(\frac 1{1-x}\Bigr)
= f\Bigl(\frac{x}{x-1}\Bigr).
\end{equation}
It suffices to check the first two equalities, the others being consequences.
The function
\begin{equation}
\label{eq:twGtw}
I = I(x) = \frac{(x^2 - x + 1)^3}{x^2(x-1)^2}
\end{equation}
belongs to $F$ as is readily seen.
Hence, the field $S = k(I)$ of all rational functions of $I$ will belong to $F$.

We contend: $F = S$ and $(E/F) = 6$.

Indeed, from Theorem~\ref{theo:onth} we obtain $(E/F) \geq 6$.
Since $S \subset F$ it suffices to prove $(E/S) \leq 6$.
Now $E = S(x)$.
It is thus sufficient to find some 6-th degree equation with coefficients in $S$ satisfied by $x$.
The following one is obviously satisfied;
\[
        (x^2 - x + 1)^3 - 1 \cdot x^2(x-1)^2 = 0.
\]

The reader will find the study of these fields a profitable exercise.
At a later occasion he will be able to derive all intermediate fields.


2)
Let $k$ be a field and $E = k(x_1, x_2, \ldots, x_n)$ the field of all rational functions of $n$ variables $x_1,x_2, \ldots, x_n$.
If $(\nu_1, \nu_2, \ldots, \nu_n)$ is a permutation of $(1,2,\ldots,n)$ we replace in each function $f(x_1,x_2,\ldots,x_n)$ of $E$ the variable $x_1$ by $x_{\nu_1}$, $x_2$ by $x_{\nu_2}$, \ldots, $x_n$ by $x_{\nu_n}$.
The mapping of $E$ onto itself obtained in this way is obviously an automorphism and we may construct $n!$ automorphisms in this fashion (including the identity).
Let $F$ be the fixed point field, that is, the set of all so-called ``symmetric functions''.
Theorem~\ref{theo:onth} shows that $(E/F) \geq n!$.
Let us introduce the polynomial:
\begin{equation}
\label{eq:twGth}
f(t) = (t-x_1)(t-x_2) \cdots (t-x_n)
= t^n + a_1 t^{n-1} + \cdots + a_n
\end{equation}
where $a_1 = -(x_1 + x_2 + \cdots + x_n)$; $a_2 = +(x_1x_2 + x_1x_3 + \cdots + x_{n-1}x_n)$, and more generally $a_i$ is $(-1)^i$ times the sum of all products of $i$ different variables of the set $x_1, x_2, \ldots, x_n$.
The functions $a_1, a_2, \ldots, a_n$ are called the elementary symmetric functions and the field $S = k(a_1, a_2, \ldots, a_n)$ of all rational functions of $a_1, a_2, \ldots, a_n$ is obviously a part of $F$.
Should we succeed in proving $(E/S) \leq n!$ we would have shown $S = F$ and $(E/F) = n!$.

We construct to this effect the following tower of fields:
\[
S = S_n
\subset S_{n-1}
\subset S_{n-2}
\subset \cdots
\subset S_2
\subset S_1 = E
\]
by the definition
\begin{equation}
\label{eq:twGfo}
S_n = S;
\quad
S_i = S(x_{i+1}, x_{i+2}, \ldots, x_n) = S_{i+1}(x_{i+1}).
\end{equation}
It would be sufficient to prove $(S_{i-1} / S_i) \leq i$ or that the generator $x_i$ for $S_{i-1}$ out of $S_i$ satisfies an equation of degree $i$ with coefficients in $S_i$.

Such an equation is easily constructed.
Put
\begin{equation}
\label{eq:twGfi}
F_i(t)
= \frac{f(t)}{(t-x_{i+1})(t-x_{i+2}) \cdots (t-x_{i+n})}
= \frac{F_{i+1}(t)}{(t-x_{i+1})}
\end{equation}
and $F_n(t) = f(t)$.
Performing the division we see that $F_i(t)$ is a polynomial in $t$ of degree $i$ whose highest coefficient is $1$ and whose coefficients are polynomials in the variables $a_1, a_2, \ldots, a_n$ and $x_{i+1}, x_{i+2}, \ldots, x_n$.
Only integers enter as coefficients in these expressions.
Now $x_i$ is obviously a root of $F_i(t) = 0$.

Now let $g(x_1, x_2, \ldots, x_n)$ be a \emph{polynomial} in $x_1, x_2, \ldots, x_n$.
Since $F_1(x_1) = 0$ is of first degree in $x_1$, we can express $x_1$ as a polynomial of the $a_i$ and of $x_2, x_3, \ldots, x_n$.
We introduce this expression in $g(x_1,x_2,\ldots,x_n)$.
Since $F_2(x_2) = 0$ we can express $x_2^2$ or higher powers as polynomials in $x_3, \ldots, x_n$ and the $a_i$.
Since $F_3(x_3) = 0$ we can express $x_3^3$ and higher powers as polynomials in $x_4, x_5, \ldots, x_n$ and the $a_i$.
Introducing these expressions in $g(x_1, x_2, \ldots, x_n)$ we see that we can express it as a polynomial in the $x_\nu$ and the $a_\nu$ such that the degree in $x_i$ is below $i$.
So $g(x_1, x_2, \ldots, x_n)$ is a linear combination of the following $n!$ terms:
\begin{equation}
\label{eq:twGsi}
x_1^{\nu_1} x_2^{\nu_2} \cdots x_n^{\nu_n}
\quad
\text{where each $\nu_i \leq i-1$.}
\end{equation}
The coefficients of these terms are polynomials in the $a_i$.
Since the expressions \eqref{eq:twGsi} are linearly independent in $S$ (this is our previous result), the expression is unique.

This is a \emph{generalization} of the theorem of symmetric functions in its usual form.
The latter says that a symmetric polynomial can be written as a polynomial in $a_1, a_2, \ldots, a_n$.
Indeed, if $g(x_1, \ldots, x_n)$ is symmetric we have already an expression as linear combination of the terms \eqref{eq:twGsi} where only the term $1$ corresponding to $\nu_1 = \nu_2 = \cdots = \nu_n = 0$ has a coefficient $\not= 0$ in $S$, namely $g(x_1, \ldots, x_n)$.
So $g(x_1, x_2, \ldots, x_n)$ is a polynomial in $a_1, a_2, \ldots, a_n$.

But our theorem gives an expression of any polynomial, symmetric or not.



\subsection{Normal extensions}

An extension field $E$ of $F$ is called a \emph{normal} extension if the group $G$ of automorphisms of $E$ which leave $F$ fixed has $F$ for its fixed field, and $(E/F)$ is finite.

Although the result in Theorem~\ref{theo:onth} cannot be sharpened in general, there is one case in which the equality sign will always occur, namely, in the case in which $\sigma_1, \sigma_2, \ldots, \sigma_n$ is a set of automorphisms which form a group.
We prove:

\begin{theo}
\label{theo:onfo}
If $\sigma_1, \sigma_2, \ldots, \sigma_n$ is a group of automorphisms of a field $E$ and if $F$ is the fixed field of $\sigma_1, \sigma_2, \ldots, \sigma_n$, then $(E/F) = n$.
\end{theo}

If $\sigma_1, \sigma_2, \ldots, \sigma_n$ is a group, then the identity occurs, say, $\sigma_1 = 1$.
The fixed field consists of those elements $x$ which are not moved by any of the $\sigma$'s, i.e., $\sigma_i(x) = x$, $i = 1,2,\ldots,n$.
Suppose that $(E/F) > n$.
Then there exist $n+1$ elements $\alpha_1, \alpha_2, \ldots, \alpha_{n+1}$ of $E$ which are linearly independent with respect to $F$.
By Theorem~\ref{theo:on}, there exists a non-trivial solution in $E$ to the system of equations
\begin{equation}
\tag{'}
\begin{aligned}
x_1 \sigma_1(\alpha_1) + 
x_2 \sigma_1(\alpha_2) + 
\cdots +
x_{n+1} \sigma_{1}(\alpha_{n+1}) &= 0
\\
x_1 \sigma_2(\alpha_1) + 
x_2 \sigma_2(\alpha_2) + 
\cdots +
x_{n+1} \sigma_{2}(\alpha_{n+1}) &= 0
\\
\cdots &
\\
x_1 \sigma_{n}(\alpha_1) + 
x_2 \sigma_{n}(\alpha_2) + 
\cdots +
x_{n+1} \sigma_{n}(\alpha_{n+1}) &= 0.
\end{aligned}
\end{equation}
We note that the solution cannot lie in $F$, otherwise, since $\sigma_1$ is the identity, the first equation would be a dependence between $\alpha_1, \ldots, \alpha_n$.

Among all non-trivial solutions $x_1, x_2, \ldots, x_{n+1}$ we choose one which has the least number of elements different from $0$.
We may suppose this solution to be $a_1, a_2, \ldots, a_r, 0, \ldots, 0$, where the first $r$ terms are different from $0$.
Moreover, $r\not=1$ because $a_1 \sigma_1(\alpha_1) = 0$ implies $a_1 = 0$ since $\sigma_1(\alpha_1) = \alpha_1 \not= 0$.
Also, we may suppose $a_r = 1$, since if we multiply the given solution by $a_r^{-1}$ we obtain a new solution in which the $r$-th term is $1$.
Thus, we have
\begin{equation}
\tag{$*$}
a_1 \sigma_{i}(\alpha_1) + 
a_2 \sigma_{i}(\alpha_2) + 
\cdots +
a_{r-1} \sigma_{i}(\alpha_{r-1}) +
\sigma_{i}(\alpha_{r}) = 0
\end{equation}
for $i = 1,2,\ldots,n$.
Since $a_1, \ldots, a_{r-1}$ cannot all belong to $F$, one of these, say $a_1$ is in $E$ but not in $F$.
There is an automorphism $\sigma_k$ for which $\sigma_k(\alpha_1) \not= a_1$.
If we use the fact that $\sigma_1, \sigma_2, \ldots, \sigma_n$ form a group, we see $\sigma_k \cdot \sigma_1, \sigma_k \cdot \sigma_2, \ldots, \sigma_k \cdots \sigma_n$ is a permutation of $\sigma_1, \sigma_2, \ldots, \sigma_n$.
Applying $\sigma_k$ to the expressions in $(*)$ we obtain
\begin{equation*}
\sigma_k(a_1) \sigma_k \sigma_{j}(\alpha_1) + 
\sigma_k(a_2) \sigma_k \sigma_{j}(\alpha_2) + 
\cdots +
\sigma_k(a_{r-1}) \sigma_k \sigma_{j}(\alpha_{r-1}) +
\sigma_k \sigma_{j}(\alpha_{r}) = 0
\end{equation*}
for $j=1,2,\ldots,n$, so that from $\sigma_k \sigma_j = \sigma_i$
\begin{equation}
\tag{$**$}
\sigma_k(a_1) \sigma_{i}(\alpha_1) + 
\sigma_k(a_2) \sigma_{i}(\alpha_2) + 
\cdots +
\sigma_k(a_{r-1}) \sigma_{i}(\alpha_{r-1}) +
\sigma_{i}(\alpha_{r}) = 0
\end{equation}
and if we subtract $(**)$ from $(*)$ we have
\begin{equation*}
(a_1 - \sigma_k(a_1)) \sigma_{i}(\alpha_1) + 
(a_2 - \sigma_k(a_2)) \sigma_{i}(\alpha_2) + 
\cdots +
(a_{r-1} - \sigma_k(a_{r-1})) \sigma_{i}(\alpha_{r-1}) = 0
\end{equation*}
which is a non-trivial solution to the system $(')$ having fewer than $r$ elements different from $0$, contrary to the choice of $r$.


\begin{coro}
\label{coro:on}
If $F$ is the fixed field for the finite group $G$, then each automorphism $\sigma$ that leaves $F$ fixed must belong to $G$.
\end{coro}

$(E/F) = \text{order of $G$} = n$.
Assume there is a $\sigma$ not in $G$.
Then $F$ would remain fixed under the $n+1$ elements consisting of $\sigma$ and the elements of $G$, thus contradicting the corollary to Theorem~\ref{theo:onth}.

\begin{coro}
\label{coro:tw}
There are no two finite group $G_1$ and $G_2$ with the same fixed field.
\end{coro}

This follows immediately from Corollary~\ref{coro:on}.

If $f(x)$ is a polynomial in $F$, then $f(x)$ is called \emph{separable} if its irreducible factors do not have repeated roots.
If $E$ is an extension of the field $F$, the \emph{element $\alpha$ of $E$ is called separable} if it is a root of a separable polynomial $f(x)$ in $F$, and $E$ is called a \emph{separable extension} if each element of $E$ is separable.

\begin{theo}
\label{theo:onfi}
$E$ is a normal extension of $F$ if and only if $E$ is the splitting field of a separable polynomial $p(x)$ in $F$.
\end{theo}


\begin{proof}
\emph{Sufficiency.}\quad
Under the assumption that $E$ splits $p(x)$ we prove that $E$ is a normal extension of $F$.

If all roots of $p(x)$ are in $F$, then our proposition is trivial, since then $E = F$ and the only unit automorphism leaves $F$ fixed.

Let us suppose $p(x)$ has $n > 1$ roots in $E$ but not in $F$.
We make the inductive assumption that for all pairs of fields with fewer than $n$ roots of $p(x)$ outside of $F$ our proposition holds.

Let $p(x) = p_1(x) \cdot p_2(x) \cdot \cdots \cdot p_r(x)$ be a factorization of $p(x)$ into irreducible factors.
We may suppose one of these to have a degree greater than one, for otherwise $p(x)$ would split in $F$.
Suppose $\deg p_1(x) = s > 1$.
Let $\alpha_1$ be a root of $p_1(x)$.
Then $(F(\alpha_1)/F) = \deg p_1(x)$.
If we consider $F(\alpha_1)$ as the new ground field, fewer roots of $p(x)$ than $n$ are outside.
From the fact that $p(x)$ lies in $F(\alpha_1)$ and $E$ is a splitting field of $p(x)$ over $F(\alpha_1)$, it follows by our inductive assumption that $E$ is a normal extension of $F(\alpha_1)$.
Thus, each element in $E$ which is not in $F(\alpha_1)$ is moved by at least one automorphism which leaves $F(\alpha_1)$ fixed.

$p(x)$ being separable, the roots $\alpha_1, \alpha_2, \ldots, a_s$ of $p(x)$ are distinct elements of $E$.
By Theorem~\ref{theo:ei} there exist isomorphisms $\sigma_1, \sigma_2, \ldots, \sigma_s$ mapping $F(\alpha_1)$ on $F(\alpha_1), F(\alpha_2), \ldots, F(\alpha_s)$, respectively, which are each the identity on $F$ and map $\alpha_1$ on $\alpha_1, \alpha_2, \ldots, \alpha_s$, respectively.
We now apply Theorem~\ref{theo:onze}.
$E$ is a splitting field of $p(x)$ in $F(\alpha_1)$ and is also a splitting field of $p(x)$ in $F(\alpha_i)$.
Hence, the isomorphism $\sigma_i$, which makes $p(x)$ in $F(\alpha_1)$ correspond to the same $p(x)$ in $F(\alpha_i)$, can be extended to an isomorphic mapping of $E$ into $E$, that is, to an automorphism of $E$ that we denote again by $\sigma_i$.
Hence $\sigma_1,\sigma_2,\ldots,\sigma_s$ are automorphisms of $E$ that leave $F$ fixed and map $\alpha_1$ onto $\alpha_1,\alpha_2,\ldots,\alpha_s$.

Now let $\theta$ be an element that remains fixed under all automorphisms of $E$ that leave $F$ fixed.
We know already that it is in $F(\alpha_1)$ and hence has the form
\[
\theta = c_0 + c_1 \alpha_1 + c_2 \alpha_1^2 + \cdots + c_{n-1} \alpha_1^{n-1}
\]
where the $c_i$ are in $F$.
If we apply $\sigma_i$ to this equation we get, since $\sigma_i(\theta) = \theta$:
\[
\theta = c_0 + c_1 \alpha_i + c_2 \alpha_i^2 + \cdots + c_{n-1} \alpha_i^{n-1}
\]

The polynomial $c_{n-1}x^{n-1} + c_{n-2}x^{n-2} + \cdots + c_1 x + (c_0 - \theta)$ has therefore the $s$ distinct roots $\alpha_1,\alpha_2,\ldots,\alpha_s$.
There are more than its degree.
So all the coefficients of it must vanish, among them $c_0 - \theta$.
This shows $\theta$ is in $F$.

\emph{Necessity.}\quad
If $E$ is a normal extension of $F$, then $E$ is the splitting field of a separable polynomial $p(x)$.
We first prove the following.

\begin{lemm*}
If $E$ is a normal extension of $F$, then $E$ is a separable extension of $F$.
Moreover any element of $E$ is a root of an equation over $F$ which splits completely in $E$.
\end{lemm*}

\begin{proof}
Let $\sigma_1,\sigma_2, \ldots,\sigma_s$ be the group $G$ of automorphisms of $E$ whose fixed field is $F$.
Let $\alpha$ be an element of $E$, and let $\alpha, \alpha_2, \alpha_3, \ldots, \alpha_r$ be the set of distinct elements in the sequence $\sigma_1(\alpha), \sigma_2(\alpha), \ldots, \sigma_s(\alpha)$.
Since $G$ is a group,
\[
\sigma_j(\alpha_i)
= \sigma_j(\sigma_k(\alpha))
= \sigma_j \sigma_k(\alpha)
= \sigma_m(\alpha)
= \alpha_n.
\]
Therefore, the elements $\alpha, \alpha_2, \ldots, \alpha_r$ are permuted by the automorphisms of $G$.
The coefficients of the polynomial $f(x) = (x-\alpha)(x-\alpha_2) \cdots (x-\alpha_r)$ are left fixed by each automorphism of $G$, since in its factored form the factors of $f(x)$ are only permuted.
Since the only elements of $E$ which are left fixed by all the automorphisms of $G$ belong to $F$, $f(x)$ is a polynomial in $F$.
If $g(x)$ is a polynomial in $F$ which also has $\alpha$ as a root, then applying the automorphisms of $G$ to the expression $g(\alpha) = 0$ we obtain $g(\alpha_i) = 0$, so that the degree of $g(x) \geq s$.
Hence $f(x)$ is irreducible, and the lemma is established.
\end{proof}

To complete the proof of the theorem, let $\omega_1,\omega_2, \ldots, \omega_t$ be a generating system for the vector space $E$ over $F$.
Let $f_i(x)$ be the separable polynomial having $\omega_i$ as a root.
Then $E$ is the splitting field of $p(x) = f_1(x) \cdot f_2(x) \cdot \cdots \cdot f_r(x)$.
\end{proof}


If $f(x)$ is a polynomial in a field $F$, and $E$ the splitting field of $f(x)$, then we shall call the group of automorphisms of $E$ over $F$ the \emph{group of the equation $f(x) = 0$}.
We come now to a theorem known in algebra as the \emph{Fundamental Theorem of Galois Theory} which gives the relationship between the structure of a splitting field and its group of automorphisms.


\begin{theo}[Fundamental Theorem]
\label{theo:onsi}
If $p(x)$ is a separable polynomial in a field $F$, and $G$ the group of the equation $p(x) = 0$ where $E$ is the splitting field of $p(x)$, then:
\begin{enumerate}
\item
Each intermediate field, $B$, is the fixed field for a subgroup $G_B$ of $G$, and distinct subgroups have distinct fixed fields.
We say $B$ and $G_B$ ``belong'' to each other.

\item
The intermediate field $B$ is a normal extension of $F$ if and only if the subgroup $G_B$ is a normal subgroup of $G$.
In this case the group of automorphisms of $B$ which leaves $F$ fixed is isomorphic to the factor group $G/G_B$.

\item
For each intermediate field $B$, we have $(B/F) = \text{the index of $G_B$}$ and $(E/B) = \text{the order of $G_B$}$.
\end{enumerate}
\end{theo}


\begin{proof}
The first part of the theorem comes from the observation that $E$ is a splitting field for $p(x)$ when $p(x)$ is taken to be in any intermediate field.
Hence, $E$ is a normal extension of each intermediate field $B$, so that $B$ is the fixed field of the subgroup of $G$ consisting of the automorphisms that leave $B$ fixed.
That distinct subgroups have distinct fixed fields is stated in Corollary~\ref{coro:tw} to Theorem~\ref{theo:onfo}.

Let $B$ be any intermediate field.
Since $B$ is the fixed field for the subgroup $G_B$ of $G$, by Theorem~\ref{theo:onfo} we have $(E/B) = \text{order of $G_B$}$.
Let us call $o(G)$ the order of a group $G$ and $i(G)$ its index.
Then $o(G) = o(G_B) \cdot i(G_B)$.
But $(E/F) = o(G)$, and $(E/F) = (E/B) \cdot (B/F)$ from which $(B/F) = i(G_B)$, which proves the third part of the theorem.

The number $i(G_B)$ is equal to the number of left cosets of $G_B$.
The elements of $G$, being automorphisms of $E$, are isomorphisms of $B$; that is, they map $B$ isomorphically into some other subfield of $E$ and are the identity on $F$.
The elements of $G$ in any one coset of $G_B$ map $B$ in the same way.
For let $\sigma \cdot \sigma_1$ and $\sigma \cdot \sigma_2$ be two elements of the coset $\sigma G_B$. Since $\sigma_1$ and $\sigma_2$ leave $B$ fixed, for each $\alpha$ in $B$ we have $\sigma \sigma_1(\alpha) = \sigma(\alpha) = \sigma \sigma_2(\alpha)$.
Elements of different cosets give different isomorphisms, for if $\sigma$ and $\tau$ give the same isomorphism, $\sigma(\alpha) = \tau(\alpha)$ for each $\alpha$ in $B$, then $\sigma^{-1}\tau(\alpha) = \alpha$ for each $\alpha$ in $B$.
Hence, $\sigma^{-1}\tau = \sigma_1$, where $\sigma_1$ is an element of $G_B$.
But then $\tau = \sigma \sigma_1$ and $\tau G_B = \sigma \sigma_1 G_B = \sigma G_B$ so that $\sigma$ and $\tau$ belong to the same coset.

Each isomorphism of $B$ which is the identity on $F$ is given by an automorphism belonging to $G$.
For let $\sigma$ be an automorphism mapping $B$ on $B'$ and the identity on $F$.
Then under $\sigma$, $p(x)$ corresponds to $p(x)$ and $E$ is the splitting field of $p(x)$ in $B$ and of $p(x)$ in $B'$.
By Theorem~\ref{theo:onze}, $\sigma$ can be extended to an automorphism $\sigma'$ of $E$, and since $\sigma'$ leaves $F$ fixed it belongs to $G$.
Therefore, the number of distinct isomorphisms of $B$ is equal to the number of cosets of $G_B$ and is therefore equal to to $(B/F)$.

The field $\sigma B$ onto which $\sigma$ maps $B$ has obviously $\sigma G_B \sigma^{-1}$ as its corresponding group, since the elements of $\sigma B$ are left invariant by precisely this group.

If $B$ is a normal extension of $F$, the number of distinct automorphisms of $B$ which leave $F$ fixed is $(B/F)$ by Theorem~\ref{theo:onfo}.
Conversely, if the number of automorphisms is $(B/F)$ then $B$ is a normal extension, because if $F'$ is the fixed field of all these automorphisms, then $F \subset F' \subset B$, and by Theorem~\ref{theo:onfo}, $(B/F')$ is equal to the number of automorphisms in the group, hence $(B/F') = (B/F)$.
From $(B/F) = (B/F') (F'/F)$ we have $(F'/F) = 1$ or $F = F'$.
Thus, $B$ is a normal extension of $F$ if and only if the number of automorphisms of $B$ is $(B/F)$.

$B$ is a normal extension of $F$ if and only if each isomorphism of $B$ into $E$ is an automorphism of $B$.
This follows from the fact that each of the above conditions are equivalent to the assertion that there are the same number of isomorphisms and automorphisms.
Since, for each $\sigma$, $B = \sigma B$ is equivalent to $\sigma G_B \sigma^{-1} \subset G_B$, we can finally say that $B$ is a normal extension of $F$ if and only if $G_B$ is a normal subgroup of $G$.

As we have shown, each isomorphism of $B$ is described by the effect of the elements of some left coset of $G_B$.
If $B$ is a normal extension these isomorphisms are all automorphisms, but in this case the cosets are elements of the factor group $G/G_B$.
Thus, each automorphism of $B$ corresponds uniquely to an element of $G/G_B$ and conversely.
Since multiplication in $G/G_B$ is obtained by iterating the mappings, the correspondence is an isomorphism between $G/G_B$ and the group of automorphisms of $B$ which leave $F$ fixed.
This completes the proof of Theorem~\ref{theo:onsi}.
\end{proof}


\subsection{Finite fields}

It is frequently necessary to know the nature of a finite subset of a field which under multiplication in the field is a group.
The answer to this question is particularly simple.


\begin{theo}
\label{theo:onse}
If $S$ is a finite subset ($\not= 0$) of a field $F$ which is a group under multiplication in $F$, then $S$ is a cyclic group.
\end{theo}


The proof is based on the following lemmas for abelian groups.


\begin{lemm}
\label{lemm:on}
If in an abelian group $A$ and $B$ are two elements of orders $a$ and $b$, and if $c$ is the least common multiple of $a$ and $b$, then there is an element $C$ of order $c$ in the group.
\end{lemm}

\begin{proof}
(a) If $a$ and $b$ are relatively prime, $C = AB$ has the required order $ab$.
The order of $C^a = B^a$ is $b$ and therefore $c$ is divisible by $b$.
Similarly it is divisible by $a$.
Since $C^{ab} = 1$ it follows $c = ab$.

(b) If $d$ is a divisor of $a$, we can find in the group an element of order $d$.
Indeed $A^{a/d}$ is this element.

(c) Now let us consider the general case.
Let $p_1, p_2, \ldots, p_r$ be the prime numbers dividing either $a$ or $b$ and let
\begin{align*}
a &= p_1^{n_1} p_2^{n_2} \cdots p_r^{n_r},
\\
b &= p_1^{m_1} p_2^{m_2} \cdots p_r^{m_r}.
\end{align*}
Call $t_i$ the larger of the two numbers $n_i$ and $m_i$.
Then
\[
c = p_1^{t_1} p_2^{t_2} \cdots p_r^{t_r}.
\]
According to (b) we can find in the group an element of order $p_i^{n_i}$ and one of order $p_i^{m_i}$.
Thus there is one of order $p_i^{t_i}$.
Part (a) shows that the product of these elements will have the desired order $c$.
\end{proof}


\begin{lemm}
\label{lemm:tw}
If there is an element $C$ in an abelian group whose order $c$ is maximal (as is always the case if the group is finite) then $c$ is divisible by the order $a$ of every element $A$ in the group; hence $x^c = 1$ is satisfied by each element in the group.
\end{lemm}

\begin{proof}
If $a$ does not divide $c$, the greatest common multiple of $a$ and $c$ would be larger than $c$ and we could find an element of that order, thus contradicting the choice of $c$.
\end{proof}


\begin{proof}[Proof of Theorem~\ref{theo:onse}]
Let $n$ be the order of $S$ and $r$ the largest order occurring in $S$.
Then $x^r - 1 = 0$ is satisfied for all elements of $S$.
Since this polynomial of degree $r$ in the field cannot have more than $r$ roots, it follows that $r \geq n$.
On the other hand $r \leq n$ because the order of each element divides $n$.
$S$ is therefore a cyclic group consisting of $1, \epsilon, \epsilon^2, \ldots, \epsilon^{n-1}$ where $\epsilon^n = 1$.
\end{proof}


Theorem~\ref{theo:onse} could also have been based on the decomposition theorem for abelian groups having a finite number of generators.
Since this theorem will be needed later, we interpolate a proof of it here.


Let $G$ be an abelian group, with group operation written as $+$.
The elements $g_1, \ldots, g_k$ will be said to generate $G$ if each element $g$ of $G$ can be written as a sum of multiples of $g_1, \ldots, g_k$, $g = n_1g_1 + \cdots + n_k g_k$.
If no set of fewer than $k$ elements generate $G$, then $g_1, \ldots, g_k$ will be called a minimal generating system.
Any group having a finite generating system admits a minimal generating system.
In particular, a finite group always admits a minimal generating system.

From the identity $n_1(g_1 + m g_2) + (n_2 - n_1m) g_2 = n_1 g + n_2 g$ it follows that if $g_1, g_2, \ldots, g_k$ generate $G$, also $g_1 + m g_2, g_2, \ldots, g_k$ generate $G$.

An equation $m_1 g_1 + m_2 g_2 + \ldots + m_k g_k = 0$ will be called a relation between the generators, and $m_1, \ldots, m_k$ will be called the coefficients in the relation.

We shall say that the abelian group $G$ is the direct product of its subgroups $G_1, G_2, \ldots, G_k$ if each $g \in G$ is uniquely representable as a sum $g = x_1 + x_2 + \ldots + x_k$, where $x_i \in G_i$, $i=1,\ldots,k$.


\begin{theo*}[Decomposition Theorem]
Each abelian group having a finite number of generators is the direct product of cyclic subgroups $G_1, \ldots, G_n$, where the order of $G_i$ divides the order of $G_{i+1}$, $i=1,\ldots,n-1$ and $n$ is the number of elements in a minimal generating system.
($G_r, G_{r+1}, \ldots, G_n$ may each be infinite, in which case, to be precise $o(G_i) \mid o(G_{i+1})$ for $i=1,2,\ldots,r-2$.)
\end{theo*}


\begin{proof}
We assume the theorem true for all groups having minimal generating systems of $k-1$ elements.
If $n=1$ the group is cyclic and the theorem trivial.
Now suppose $G$ is an abelian group having a minimal generating system of $k$ elements.
If no minimal generating system satisfies a non-trivial relation, then let $g_1,g_2, \ldots, g_k$ be a minimal generaing system and $G_1, G_2, \ldots, G_k$ be the cyclic groups generated by them.
For each $g \in G$, $g = n_1g_1 + \cdots + n_kg_k$ where the expression is unique; otherwise we should obtain a relation.
Thus the theorem would be true.
Assume now that some non-trivial relations hold for some minimal generating systems.
Among all relations between minimal generating systems, let
\begin{equation}
\label{eq:Ion}
m_1 g_1 + \cdots + m_k g_k = 0
\end{equation}
be a relation in which the smallest positive coefficient occurs.
After an eventual reordering of the generators we can suppose this coefficient to be $m_1$.
In any other relation between $g_1, \ldots, g_k$,
\begin{equation}
\label{eq:Itw}
n_1g_1 + \cdots + n_kg_k = 0
\end{equation}
we must have $m_1 \mid n_1$.
Otherwise $n_1 = q m_1 + r$, $0 < r < m_1$, and $q$ times relation \eqref{eq:Ion} subtracted from relation \eqref{eq:Itw} would yield a relation with a coefficient $r < m_1$.
Also in relation \eqref{eq:Ion} we must have $m_1 \mid m_i$, $i = 2, \ldots, k$.
For suppose $m_1$ does not divide one coefficient, say $m_2$.
Then $m_2 = q m_1 + r$, $0 < r < m_1$.
In the generating system $g_1+g_2, g_2,\ldots,g_k$ we should have a relation $m_1(g_1 + qg_2) + rg_2 + m_3g_3 + \cdots + m_k g_k = 0$ where the coefficient $r$ contradicts the choice of $m_1$.
Hence $m_2 = q_2 m_1, m_3 = q_3 m_1, \ldots, m_k = q_k m_1$.
The system $\bar g_1 = g_1 + q_2g_2 + \cdots + q_k g_k, g_2, \ldots, g_k$ is minimal generating, and $m_1 \bar g_1 = 0$.
In any relation $0 = n_1 \bar g_1 + n_2 g_2 + \cdots + n_k g_k$ since $m_1$ is a coefficient in a relation between $\bar g_1, g_2, \ldots, g_k$ our previous argument yields $m_1 \mid n_1$, and hence $n_1 \bar g_1 = 0$.

Let $G'$ be the subgroup of $G$ generated by $g_2, \ldots, g_k$ and $G_1$ the cyclic group of order $m_1$ generated by $\bar g_1$.
Then $G$ is the direct product of $G_1$ and $G'$.
Each element $g$ of $G$ can be written
\[
g = n_1 \bar g_1 + n_2 g_2 + \cdots + n_k g_k
= n_1 \bar g_1 + g'.
\]
The representation is unique, since $n_1 \bar g_1 + g' = n_1' g_2 + g''$ implies the relation $(n_1 - n_1') \bar g_1 + (g' - g'') = 0$, hence $(n_1 - n_1') \bar g_1 = 0$, so that $n_1 \bar g_1 = n_1' \bar g_1$, and also $g' = g''$.

By our inductive hypothesis, $G'$ is the direct product of $k-1$ cyclic groups generated by the elements $\bar g_2, \bar g_3, \ldots, \bar g_k$ whose respective orders $t_2, \ldots, t_k$ satisfy $t_i \mid t_{i+1}$, $i = 2,\ldots,k-1$.
The preceding argument applied to the generators $\bar g_1, \bar g_2, \ldots, \bar g_k$ yields $m_1 \mid t_2$, from which the theorem follows.
\end{proof}


By a \emph{finite field} is meant one having only a finite number of elements.


\begin{coro*}
\label{coro:p53}
The non-zero elements of a finite field form a cyclic group.
\end{coro*}


If $a$ is an element of a field $F$, let us denote the $n$-fold of $a$, i.e., the element of $F$ obtained by adding $a$ to itself $n$ times, by $na$.
It is obvious that $n\cdot(m\cdot a) = (nm) \cdot a$ and $(n \cdot a) (m \cdot b) = nm \cdot ab$.
If for one element $a \not= 0$, there is an integer $n$ such that $n \cdot a = 0$, then $n \cdot b = 0$ for each $b$ in $F$, since $n \cdot b = (n \cdot a)(a^{-1}b) = 0$.
If there is a positive integer $p$ such that $p \cdot a = 0$ for each $a$ in $F$, and if $p$ is the smallest integer with this property, then $F$ is said to have the \emph{characteristic $p$}.
If no such positive integer exists then we say $F$ has characteristic $0$.
\emph{The characteristic of a field is always a prime number}, for if $p = r \cdot s$ then $pa = rs \cdot a = r \cdot (s \cdot a)$.
However $s \cdot a = b \not= 0$ if $a \not= 0$ and $r \cdot b \not= 0$ since both $r$ and $s$ are less than $p$, so that $pa \not= 0$ contrary to the definition of the characteristic.
If $na = 0$ for $a \not= 0$, then $p$ divides $n$, for $n = qp + r$ where $0 \leq r < p$ and $na = (qp + r)a = qpa + ra$.
Hence $na = 0$ implies $ra = 0$ and from the definition of the characteristic since $r < p$, we must have $r = 0$.

If $F$ is a finite field having $q$ elements and $E$ an extension of $F$ such that $(E/F) = n$, then $E$ has $q^n$ elements.
For if $\omega_1, \omega_2, \ldots, \omega_n$ is a basis of $E$ over $F$, each element of $E$ can be uniquely represented as a linear combination $x_1 \omega_1 + x_2 \omega_2 + \cdots + x_n \omega_n$ where the $x_i$ belong to $F$.
Since each $x_i$ can assume $q$ values in $F$, there are $q^n$ distinct possible choices of $x_1,\ldots,x_n$ and hence $q^n$ distinct elements of $E$.
$E$ is finite, hence, \emph{there is an element $\alpha$ of $E$ so that $E = F(\alpha)$}.
(The non-zero elements of $E$ form a cyclic group generated by $\alpha$.)

If we denote by $P = \{0,1,2,\ldots,p-1\}$ the set of multiples of the unit element in a field $F$ of characteristic $p$, then $P$ is a subfield of $F$ having $p$ distinct elements.
In fact, $P$ is isomorphic to the field of integers reduced mod $p$.
If $F$ is a finite field, then the degree of $F$ over $P$ is finite, say $(F/P) = n$, and $F$ contains $p^n$ elements.
In other words, \emph{the order of any finite field is a power of the characteristic}.


If $F$ and $F'$ are two finite fields having the same order $q$, then by the preceding, they have the same characteristic since $q$ is a power of the characteristic.
The multiples of the unit in $F$ and $F'$ form two fields $P$ and $P'$ which are isomorphic.

The non-zero elements of $F$ and $F'$ form a group of order $q-1$ and, therefore, satisfy the equation $x^{q-1} - 1 = 0$.
The fields $F$ and $F'$ are splitting fields of the equation $x^{q-1} = 1$ considered as lying in $P$ and $P'$ respectively.
By Theorem~\ref{theo:onze}, the isomorphism between $P$ and $P'$ can be extended to an isomorphism between $F$ and $F'$.
We have thus proved:


\begin{theo}
\label{theo:onei}
Two finite fields having the same number of elements are isomorphic.
\end{theo}



\paragraph{Differentation}
If $f(x) = a_0 + a_1x + \cdots + a_nx^n$ is a polynomial in a field $F$, then we define $f' = a_1 + 2a_2x + \cdots + n a_n x^{n-1}$.
The reader may readily verify that for each pair of polynomials $f$ and $g$ we have
\begin{align*}
(f+g)' &= f' + g',
\\
(fg)' &= fg' + f'g,
\\
(f^n)' &= nf^{n-1} \cdot f'.
\end{align*}

\begin{theo}
\label{theo:onni}
The polynomial $f$ has repeated roots if and only if in the splitting field $E$ the polynomials $f$ and $f'$ have a common root.
This condition is equivalent to the assertion that $f$ and $f'$ have a common factor of degree greater than $0$ in $F$.
\end{theo}

\begin{proof}
If $\alpha$ is a root of multiplicity $k$ of $f(x)$ then $f = (x-\alpha)^k Q(x)$ where $Q(\alpha) \not= 0$.
This gives
\[
f' = (x - \alpha)^k Q'(x) + k (x - \alpha)^{k-1} Q(x)
= (x - \alpha)^{k-1}\bigl(
(x - \alpha) Q'(x) - k Q(x)
\bigr).
\]
If $k > 1$, then $\alpha$ is a root of $f'$ of multiplicity at least $k-1$.
If $k=1$, then $f'(x) = Q(x) + (x -\alpha) Q'(x)$ and $f'(\alpha) = Q(\alpha) \not= 0$.
Thus, $f$ and $f'$ have a root $\alpha$ in common if and only if $\alpha$ is a root of $f$ of multiplicity greater than $1$.

If $f$ and $f'$ have a root $\alpha$ in common then the irreducible polynomial in $F$ having $\alpha$ as root divides both $f$ and $f'$.
Conversely, any root of a factor common to both $f$ and $f'$ is a root of $f$ and $f'$.
\end{proof}


\begin{coro*}
If $F$ is a field of characteristic $0$ then each irreducible polynomial in $F$ is separable.
\end{coro*}

\begin{proof}
Suppose to the contrary that the irreducible polynomial $f(x)$ has a root $\alpha$ of multiplicity greater than $1$.
Then, $f'(x)$ is a polynomial which is not identically zero (its leading coefficient is a multiple of the leading coefficient of $f$ and is not zero since the characteristic is $0$) and of degree 1 less than the degree of $f(x)$.
But $\alpha$ is also a root of $f'(x)$ which contradicts the irreducibility of $f(x)$.
\end{proof}


\subsection{Roots of unity}

If $F$ is a field having any characteristic $p$, and $E$ the splitting field of the polynomial $x^n - 1$ where $p$ does not divide $n$, then we shall refer to $E$ as \emph{the field generated out of $F$ by the adjunction of a primitive $n$-th root of unity}.

The polynomial $x^n - 1$ does not have repeated roots in $E$, since its derivative, $n x^{n-1}$, has only the root $0$ and has, therefore, no roots in common with $x^n - 1$.
Thus, $E$ is a normal extension of $F$.
If $\epsilon_1, \epsilon_2, \ldots, \epsilon_n$ are the roots of $x^n - 1$ in $E$, they form a group under multiplication and by Theorem~\ref{theo:onse} this group will be cyclic.
If $1, \epsilon, \epsilon^2, \ldots, \epsilon^{n-1}$ are the elements of this group, we shall call $\epsilon$ a primitive $n$-th root of unity.
The smallest power of $\epsilon$ which is $1$ is the $n$-th.


\begin{theo}
\label{theo:twze}
If $E$ is the field generated from $F$ by a primitive $n$-th root of unity, then the group $G$ of $E$ over $F$ is abelian for any $n$ and cyclic if $n$ is a prime power.
\end{theo}


\begin{proof}
We have $E = F(\epsilon)$, since the roots of $x^n - 1$ are powers of $\epsilon$.
Thus, if $\sigma$ and $\tau$ are distinct elements of $G$, $\sigma(\epsilon) \not= \tau(\epsilon)$.
But $\sigma(\epsilon) = \epsilon^{n_\sigma}$ where $n_\sigma$ is an integer $1 \leq n_\sigma < n$.
Moreover, $\tau \sigma(\epsilon) = \tau(\epsilon^{n_\sigma}) = (\tau(\epsilon))^{n_\sigma} = \epsilon^{n_\tau \cdot n_\sigma} = \sigma \tau(\epsilon)$.
Thus, $n_{\sigma\tau} = n_\sigma n_\tau \mod n$.
Thus, the mapping of $\sigma$ on $n_\sigma$ is a homomorphism of $G$ into a multiplicative subgroup of the integers mod $n$.
Since $\tau \not= \sigma$ implies $\tau(\epsilon) \not= \sigma(\epsilon)$, it follows that $\tau \not= \sigma$ implies $n_{\sigma} \not= n_\tau \mod n$.
Hence, the homomorphism is an isomorphism.
If $n$ is a prime number, the multiplicative group of numbers forms a cyclic group.
\end{proof}


\subsection{Noether equations}

If $E$ is a field, and $G = (\sigma, \tau, \ldots)$ a group of automorphisms of $E$, any set of elements $x_\sigma, x_\tau, \ldots$ in $E$ will be said to provide a \emph{solution to Noether's equations} if $x_\sigma \cdot \sigma(x_\tau) = x_{\sigma\tau}$ for each $\sigma$ and $\tau$ in $G$.
If one element $x_\sigma = 0$ then $x_\tau = 0$ for each $\tau \in G$.
As $\tau$ traces $G$, or assumes all values in $G$, and in the above equation $x_{\sigma\tau} = 0$ when $x_\sigma = 0$.
Thus, in any solution of the Noether equations no element $x_\sigma = 0$ unless the solution is completely trivial.
We shall assume in the sequel that the trivial solution has been excluded.


\begin{theo}
\label{theo:21}
The system $x_\sigma, x_\tau, \ldots$ is a solution to Noether's equations if and only if there exists an element $\alpha \in E$, such that $x_\sigma = \alpha / \sigma(\alpha)$ for each $\sigma$.
\end{theo}


\begin{proof}
For any $\alpha$, it is clear that $x_\sigma = \alpha / \sigma(\alpha)$ is a solution to the equations, since
\[
\alpha / \sigma(\alpha) \cdot \sigma(\alpha / \tau(\alpha))
= \alpha / \sigma(\alpha) \cdot \sigma(\alpha) / \sigma\tau(\alpha)
= \alpha / \sigma\tau(\alpha).
\]
Conversely, let $x_\sigma, x_\tau, \ldots$ be a non-trivial solution.
Since the automorphisms $\sigma, \tau, \ldots$ are distinct they are linearly independent, and the equation $x_\sigma \cdot \sigma(z) + x_\tau\tau(z) + \cdots = 0$ does not hold identically.
Hence, there is an element $a$ in $E$ such that $x_\sigma \sigma(a) + x_\tau \tau(a) + \cdots = \alpha \not= 0$.
Applying $\sigma$ to $\alpha$ gives
\[
\sigma(\alpha)
= \sum_{\tau \in G} x_\sigma \sigma(x_\tau) \, \sigma\tau(a).
\]
Replacing $x_\sigma \cdot \sigma(x_\tau)$ by $x_{\sigma\tau}$ and noting that $\sigma\tau$ assumes all values in $G$ when $\tau$ does, we have
\[
x_\sigma \cdot \sigma(\alpha)
= \sum_{\tau \in G} x_{\tau} \tau(a) 
= \alpha
\]
so that
\[
x_\sigma = \alpha / \sigma(\alpha).
\qedhere
\]
\end{proof}


A solution to the Noether equations defines a mapping $C$ of $G$ into $E$, namely $C(\sigma) = x_\sigma$.
If $F$ is the fixed field of $G$, and the elements of $x_\sigma$ lie in $F$, then $C$ is a \emph{character} of $G$.
For $C(\sigma\tau) = x_{\sigma\tau} = x_\sigma \cdot \sigma(x_\tau) = x_\sigma x_\tau = C(\sigma) \cdot C(\tau)$ since $\sigma(x_\tau) = x_\tau$ if $x_\tau \in F$.
Conversely, each character $C$ of $G$ in $F$ provides a solution to the Noether equations.
Call $C(\sigma) = x_\sigma$.
Then, since $x_\tau \in F$, we have $\sigma(x_\tau) = x_\tau$.
Thus, $x_\sigma \cdot \sigma(x_\tau) = x_\sigma \cdot x_\tau = C(\sigma) \cdot C(\tau) = C(\sigma\tau) = x_{\sigma\tau}$.
We therefore have, by combining this with Theorem~\ref{theo:21}.

\begin{theo}
\label{theo:22}
If $G$ is the group of the normal field $E$ over $F$, then for each character $C$ of $G$ into $F$ there exists an element $\alpha$ in $E$ such that $C(\sigma) = \alpha / \sigma(\alpha)$ and, conversely, if $\alpha / \sigma(\alpha)$ is in $F$ for each $\sigma$, then $C(\sigma) = \alpha/\sigma(\alpha)$ is a character of $G$.
If $r$ is the least common multiple of the orders of elements of $G$, then $\alpha^r \in F$.
\end{theo}



\subsection{Kummer's fields}

If $F$ contains a primitive $n$-th root of unity, any splitting field $E$ of a polynomial $(x^n - a_1)(x^n - a_2) \cdots (x^n - a_r)$ where $a_i \in F$ for $i = 1,2,\ldots,r$ will be called a \emph{Kummer extension} of $F$, or more briefly, a \emph{Kummer field}.


If a field $F$ contains a primitive $n$-th root of unity, the number $n$ is not divisible by the characteristic of $F$.
Suppose, to the contrary, $F$ has characteristic $p$ and $n = qp$.
Then $y^p - 1 = (y-1)^p$ since in the expansion of $(y-1)^p$ each coefficient other than the first and last is divisible by $p$ and therefore is a multiple of the $p$-fold of the unit of $F$ and is thus equal to $0$.
Therefore $x^n - 1 = (x^q)^p - 1 = (x^q - 1)^p$ and $x^n - 1$ cannot have more than $q$ distinct roots.
But we assume that $F$ has a primitive $n$-th root of unity, and $1, \epsilon, \epsilon^2, \ldots, \epsilon^{n-1}$ would be $n$ distinct roots of $x^n - 1$.
It follows that $n$ is not divisible by the characteristic of $F$.
For a Kummer field $E$, none of the factors $x^n - a_i$, $a_i \not= 0$ has repeated roots since the derivative, $n x^{n-1}$, has only the root $0$ and has therefore no roots in common with $x^n - a_i$.
Therefore, the irreducible factors of $x^n - a_i$ are separable, so that \emph{$E$ is a normal extension of $F$}.

Let $\alpha_i$ be a root of $x^n - a_i$ in $E$.
If $\epsilon_1, \epsilon_2, \ldots, \epsilon_n$ are the $n$ distinct $n$-th roots of unity in $F$, then $\alpha_i \epsilon, \alpha_i \epsilon_2, \ldots, \alpha_i \epsilon_n$ will be $n$ distinct roots of $x^n - a_i$, and hence will be the roots of $x^n - a_i$, so that $E = F(\alpha_1, \alpha_2, \ldots, \alpha_n)$.
Let $\sigma$ and $\tau$ be two automorphisms in the group $G$ of $E$ over $F$.
For each $\alpha_i$, both $\sigma$ and $\tau$ map $\alpha_i$ on some other root of $x^n - a_i$.
Thus $\tau(\alpha_i) = \epsilon_{i\tau} \alpha_i$ and $\sigma(\alpha_i) = \epsilon_{i\sigma} \alpha_i$ where $\epsilon_{i\sigma}$ and $\epsilon_{i\tau}$ are $n$-th roots of unity in the basic field $F$.
It follows that $\tau(\sigma(\alpha_i)) = \tau(\epsilon_{i\sigma} \alpha_i) = \epsilon_{i\sigma} \tau(\alpha_i) = \epsilon_{i\sigma}\epsilon_{i\tau} \alpha_i = \sigma(\tau(\alpha_i))$.
Since $\sigma$ and $\tau$ are commutative over the generators of $E$, they commute over each element of $E$.
Hence, $G$ is commutative.
If $\sigma \in G$, then $\sigma(\alpha_i) = \epsilon_{i\sigma}\alpha_i$, $\sigma^2(\alpha_i) = \epsilon_{i\sigma}^2 \alpha_i$, etc.
Thus, $\sigma^{n_i}(\alpha_i) = \alpha_i$ for $n_i$ such that $\epsilon_{i\sigma}^{n_i} = 1$.
Since the order of an $n$-th root of unity is a divisor of $n$, we have $n_i$ a divisor of $n$ and the least common multiple $m$ of $n_1, n_2, \ldots, n_r$ is a divisor of $n$.
Since $\sigma^m(\alpha_i) = \alpha_i$ for $i = 1,2,\ldots, r$ it follows that $m$ is the order of $\sigma$.
Hence, the order of each element of $G$ is a divisor of $n$ and, therefore, the least common multiple $r$ of the orders of the elements of $G$ is a divisor of $n$.
If $\epsilon$ is a primitive $n$-th root of unity, then $\epsilon^{n/r}$ is a primitive $r$-th root of unity.
These remarks can be summarized in the following:


\begin{theo}
\label{theo:23}
If $E$ is a Kummer field, i.e., a splitting field of $p(x) = (x^n - a_1)(x^n - a_2) \cdots (x^n - a_r)$ where $a_i$ lie in $F$, and $F$ contains a primitive $n$-th root of unity, then:
\begin{enumerate}
\item
\label{theo:23a}
$E$ is a normal extension of $F$;

\item
\label{theo:23b}
the group $G$ of $E$ over $F$ is abelian;

\item
\label{theo:23c}
the least common multiple of the orders of the elements of $G$ is a divisor of $n$.
\end{enumerate}
\end{theo}


\begin{coro*}
If $E$ is the splitting field of $x^p - a$, and $F$ contains a primitive $p$-th root of unity where $p$ is a prime number, then either $E = F$ and $x^p - a$ is split in $F$, or $x^p - a$ is irreducible and its group of $E$ over $F$ is cyclic of order $p$.
\end{coro*}


\begin{proof}
The order of each element is, by Theorem~\ref{theo:23}, a divisor of $p$ and hence, if the element is not the unit its order must be $p$.
If $\alpha$ is a root of $x^p - a$, then $\alpha, \epsilon\alpha, \ldots, \epsilon^{p-1} \alpha$ are all the roots of $x^p - a$ so that $F(\alpha) = E$ and $(E/F) \leq p$.
Hence, the order of $G$ does not exceed $p$ so that if $G$ has one element different from the unit, it and its powers must constitute all of $G$.
Since $G$ has $p$ distinct elements and their behavior is determined by their effect on $\alpha$, then $\alpha$ must have $p$ distinct images.
Hence, the irreducible equation in $F$ for $\alpha$ must be of degree $p$ and is therefore $x^p - a = 0$.
\end{proof}


The properties \eqref{theo:23a}, \eqref{theo:23b} and \eqref{theo:23c} in Theorem~\ref{theo:23} actually characterize Kummer fields.


Let us suppose that $E$ is a normal extension of a field $F$, whose group $G$ over $F$ is abelian.
Let us further assume that $F$ contains a primitive $r$-th root of unity where $r$ is the least common multiple of the orders of the elements of $G$.

The group of characters $X$ of $G$ int othe group of $r$-th roots of unity is isomorphic to $G$.
Moreover, to each $\sigma \in G$, if $\sigma \not= 1$, there exists a character $C \in X$ such that $C(\sigma) \not= 1$.
Write $G$ as the direct product of the cyclic groups $G_1, G_2, \ldots, G_t$ of orders $m_1 \mid m_2 \mid \cdots \mid m_t$.
Each $\sigma \in G$ may be written $\sigma = \sigma_1^{\nu_1} \sigma_2^{\nu_2} \cdots \sigma_t^{\nu_t}$.
Call $C_i$ the character sending $\sigma_i$ into $\epsilon_i$, a primitive $m_i$-th root of unity and $\sigma_j$ into $1$ for $j\not= i$.
Let $C$ be any character.
$C(\sigma_i) = \epsilon_i^{\mu_i}$, then we have $C(\sigma) = C_1^{\mu_1} C_2^{\mu_2} \cdots C_t^{\mu_t}$
Conversely, $C_1^{\mu_1} C_2^{\mu_2} \cdots C_t^{\mu_t}$ defines a character.
Since the order of $C_i$ is $m_i$, the character group $X$ of $G$ is isomorphic to $G$.
If $\sigma \not= 1$, then in $\sigma = \sigma_1^{\nu_1} \sigma_2^{\nu_2} \cdots \sigma_t^{\nu_t}$ at least one $\nu_i$, say $\nu_1$, is not divisible by $m_i$.
Thus $C_i(\sigma) = \epsilon_1^{\nu_1} \not= 1$.

Let $A$ denote the set of those non-zero elements $\alpha$ of $E$ for which $\alpha^r \in F$ and let $F_1$ denote the non-zero elements of $F$.
It is obvious that $A$ is a multiplicative group and that $F_1$ is a subgroup of $A$.
Let $A^r$ denote the set of $r$-th powers of elements in $A$ and $F_1^r$ the set of $r$-th powers of elements of $F_1$.
The following theorem provides in most application a convenient method for computing the group $G$.


\begin{theo}
\label{theo:24}
The factor groups $A/F_1$ and $A^r/F_1^r$ are isomorphic to each other and to the groups $G$ and $X$.
\end{theo}

\begin{proof}
We map $A$ on $A^r$ by making $\alpha \in A$ correspond to $a^r \in A^r$.
If $a^r \in F_1^r$ where $a \in F_1$ then $b \in A$ is mapped on $a^r$ if and only if $b^r = a^r$., that is, if $b$ is a solution to the equation $x^r - a^r = 0$.
But $a, \epsilon a, \epsilon^2 a, \ldots, \epsilon^{r-1} a$ are distinct solutions to this equation and since $\epsilon$ and $a$ belong to $F_1$, it follows that $b$ must be one of these elements and must belong to $F_1$.
Thus, the inverse set on $A$ of the subgroup $F_1^r$ of $A^r$ is $F_1$, so that the factor groups $A/F_1$ and $A^r/F_1^r$ are isomorphic.

If $\alpha$ is an element of $A$, then $(\alpha / \sigma(\alpha))^r = \alpha^r / \sigma(\alpha^r) = 1$.
Hence, $\alpha / \sigma(\alpha)$ is an $r$-th root of unity and lies in $F_1$.
By Theorem~\ref{theo:22}, $\alpha / \sigma(\alpha)$ defines a character $C(\sigma)$ of $G$ in $F$.
We map $\alpha$ on the corresponding character $C$.
Each character $C$ is by Theorem~\ref{theo:22} the image of some $\alpha$.
Moreover, $\alpha \cdot \alpha'$ is mapped on the character $C^*(\sigma) = \alpha \cdot \alpha' / \sigma(\alpha \cdot \alpha') = \alpha \cdot \alpha' / \sigma(\alpha) \cdot \sigma(\alpha') = C(\sigma) \cdot C'(\sigma) = C \cdot C'(\sigma)$, so that the mapping is a homomorphism.
The kernel of this homomorphism is the set of those elements $\alpha$ for which $\alpha / \sigma(\alpha) = 1$ for each $\alpha$, hence is $F_1$.
It follows, therefore, that $A / F_1$ is isomorphic to $X$ and hence also to $G$.
In particular, $A/F_1$ is a finite group.
\end{proof}


We now prove the equivalence between Kummer fields and fields satisfying \eqref{theo:23a}, \eqref{theo:23b} and \eqref{theo:23c} of Theorem~\ref{theo:23}.



\begin{theo}
\label{theo:25}
If $E$ is an extension field over $F$, then $E$ is a Kummer field if and only if $E$ is normal, its group $G$ is abelian and $F$ contains a primitive $r$-th root of unity where $r$ is the least common multiple of the orders of the elements of $G$.
\end{theo}


\begin{proof}
The necessity is already contained in Theorem~\ref{theo:23}.
We prove the sufficiency.
Out of the group $A$, let $\alpha_1 F_1, \alpha_2 F_1, \ldots, \alpha_t F_1$ be the cosets of $F_1$.
Since $\alpha_i \in A$ we have $\alpha_i^r = a_i \in F$.
Thus, $\alpha_i$ is a root of the equation $x^r - a_i = 0$ and since $\epsilon \alpha_i, \epsilon^2 \alpha_i, \ldots, \epsilon^{r-1} \alpha_i$ are also roots, $x^r - a_i$ must split in $E$.
We prove that $E$ is the splitting field of $(x^r - a_1) (x^r - a_2) \cdots (x^r - a_t)$ which will complete the proof of the theorem.
To this end it suffices to show that $F(\alpha_1, \alpha_2, \ldots, \alpha_t) = E$.

Suppose that $F(\alpha_1, \alpha_2, \ldots, \alpha_t) \not= E$.
Then $F(\alpha_1, \alpha_2, \ldots, \alpha_t)$ is an intermediate field between $F$ and $E$, and since $E$ is normal over $F(\alpha_1, \alpha_2, \ldots, \alpha_t)$ there exists an automorphism $\sigma \in G$, $\sigma \not= 1$, which leaves $F(\alpha_1, \alpha_2, \ldots, \alpha_t)$ fixed.
There exists a character $C$ of $G$ for which $C(\sigma) \not= 1$.
Finally, there exists an element $\alpha$ in $E$ such that $C(\sigma) = \alpha / \sigma(\alpha) \not= 1$.
But $\alpha^r \in F_1$ by Theorem~\ref{theo:22}, hence $\alpha \in A$.
Moreover, $A \subset F(\alpha_1, \alpha_2, \ldots, \alpha_t)$ since all the cosets $\alpha_i F$ are contained in $F(\alpha_1, \alpha_2, \ldots, \alpha_t)$.
Since $F(\alpha_1, \alpha_2, \ldots, \alpha_t)$ is by assumption left fixed by $\sigma$, $\sigma(\alpha) = \alpha$ which contradicts $\alpha / \sigma(\alpha) \not= 1$.
It follows, therefore, that $F(\alpha_1, \alpha_2, \ldots, \alpha_t) = E$.
\end{proof}


\begin{coro*}
If $E$ is a normal extension of $F$, of prime order $p$, and if $F$ contains a primitive $p$-th root of unity, then $E$ is the splitting field of an irreducible polynomial $x^p - a$ in $F$.
\end{coro*}

\begin{proof}
$E$ is generated by elements $\alpha_1, \ldots, \alpha_n$ where $\alpha_i^p \in F$.
Let $\alpha_1$ be not in $F$ and let $a = \alpha_1^r$.
Then $x^p - a$ is irreducible, for otherwise $F(\alpha_1)$ would be an intermediate field between $F$ and $E$ of degree less than $p$, and by the product theorem for the degrees, $p$ would not be a prime number, contrary to assumption.
$E = F(\alpha_1)$ is the splitting field of $x^p - a$.
\end{proof}



\subsection{Simple extensions}

We consider the question of determining under what conditions an extension field is generated by a \emph{single element}, called a \emph{primitive}.
We prove the following.


\begin{theo}
\label{theo:26}
A finite extension $E$ of $F$ is primitive over $F$ if and only if there are only a finite number of intermediate fields.
\end{theo}


\begin{proof}
(a)\quad
Let $E = F(\alpha)$ and call $f(x) = 0$ the irreducible equation for $\alpha$ in $F$.
Let $B$ be an intermediate field and $g(x)$ the irreducible equation for $\alpha$ in $B$.
The coefficients of $g(x)$ adjoined to $F$ will generate a field $B'$ between $F$ and $B$.
$g(x)$ is irreducible in $B$, hence also in $B'$.
Since $E = B'(\alpha)$ we see $(E/B) = (E/B')$.
This proves $B' = B$.
So $B$ is uniquely determined by the polynomial $g(x)$.
But $g(x)$ is a divisor of $f(x)$, and there are only a finite number of possible divisors of $f(x)$ in $E$.
Hence there are only a a finite number of possible $B$'s.

(b)\quad
Assume there are only a finite number of fields between $E$ and $F$.
Should $F$ consist of only a finite number of elements, then $E$ is generated by one element according to the Corollary on page~\pageref{coro:p53}.
We may therefore assume $F$ to contain an infinity of elements.
We prove:
To any two elements $\alpha,\beta$ there is a $\gamma$ in $E$ such that $F(\alpha,\beta) = F(\gamma)$.
Let $\gamma = \alpha + a \beta$ with $a$ in $F$ but for the moment undetermined.
Consider all the fields $F(\gamma)$ obtained in this way.
Since we have an infinity of $a$'s at our disposal, we can find two, say $a_1$ and $a_2$, such that the corresponding $\gamma$'s, $\gamma_1 = \alpha + a_1 \beta$ and $\gamma_2 = \alpha + a_2 \beta$, yield the same field $F(\gamma_1) = F(\gamma_2)$.
Since both $\gamma_1$ and $\gamma_2$ are in $F(\gamma_1)$, their different (and therefore $\beta$) is in this field.
Consequently also $\gamma_1 - \alpha_1 \beta = \alpha$.
For $F(\alpha,\beta) \subset F(\gamma_1)$.
Since $F(\gamma_1) \subset F(\alpha,\beta)$ our contention is proved.
Select now $\eta$ in $E$ in such a way that $(F(\eta) / F)$ is as large as possible.
Every element $\epsilon$ of $E$ must be in $F(\eta)$ or else we could find an element $\delta$ such that $F(\delta)$ contains both $\eta$ and $\epsilon$.
This proves $E = F(\eta)$.
\end{proof}



\begin{theo}
\label{theo:27}
If $E = F(\alpha_1, \alpha_2, \ldots, \alpha_n)$ is a finite extension of the field $F$, and $\alpha_1, \alpha_2, \ldots, \alpha_n$ are separable elements in $E$, then there exists a primitive $\theta$ in $E$ such that $E = F(\theta)$.
\end{theo}


\begin{proof}
Let $f_i(x)$ be the irreducible equation of $\alpha_i$ in $F$ and let $B$ be an extension of $E$ that splits $f_1(x) f_2(x) \cdots f_n(x)$.
Then $B$ is normal over $F$ and contains, therefore, only a finite number of intermediate fields (as many as there are subgroups of $G$).
So the subfield $E$ contains only a finite number of intermediate fields.
Theorem~\ref{theo:26} now completes the proof.
\end{proof}



\subsection{Existence of a normal basis}

The following theorem is true for any field though we prove it only in the case that $F$ contains an infinity of elements.


\begin{theo}
\label{theo:28}
If $E$ is a normal extension of $F$ and $\sigma_1, \sigma_2, \ldots, \sigma_n$ are the elements of its group $G$, there is an element $\theta$ in $E$ such that the $n$ elements of $\sigma_1(\theta), \sigma_2(\theta), \ldots, \sigma_n(\theta)$ are linearly independent with respect to $F$.
\end{theo}


\begin{proof}
According to Theorem~\ref{theo:27} there is an $\alpha$ such that $E = F(\alpha)$.
Let $f(x)$ be the equation for $\alpha$, put $\sigma_i(\alpha) = \alpha_i$,
\[
g(x) = \frac{f(x)}{(x-\alpha)f'(\alpha)}
\quad\text{and}\quad
g_i(x) = \sigma_i(g(x))
= \frac{f(x)}{(x - \alpha_i)f'(\alpha_i)}.
\]
$g_i(x)$ is a polynomial in $E$ having $\alpha_k$ as a root for $k\not= i$ and hence
\begin{equation}
\label{eq:N1}
g_i(x) g_k(x) = 0
\quad
\text{$\pmod{f(x)}$ for $i \not= k$}.
\end{equation}
In the equation
\begin{equation}
\label{eq:N2}
g_1(x) + g_2(x) + \cdots + g_n(x) - 1 = 0
\end{equation}
the left side is of degree at most $n-1$.
If \eqref{eq:N2} is true for $n$ different values of $x$, the left side must be identically $0$.
Such $N$ values are $\alpha_1, \alpha_2, \ldots, \alpha_n$, since $g_i(\alpha_i) = 1$ and $g_k(\alpha_i) = 0$ for $k \not= i$.

Multiplying \eqref{eq:N2} by $g_i(x)$ and using \eqref{eq:N1} shows:
\begin{equation}
\label{eq:N3}
(g_i(x))^2 = g_i(x) \pmod{f(x)}.
\end{equation}
We next compute the determinant
\begin{equation}
\label{eq:N4}
D(x) = |\sigma_i \sigma_k (g(x)) |
\quad
i,k = 1,2,\ldots,n
\end{equation}
and prove $D(x) \not= 0$.
If we square it by multiplying column by column and compute its value mod $f(x)$ we get from \eqref{eq:N1}, \eqref{eq:N2}, \eqref{eq:N3} a determinant that has $1$ in the diagonal and $0$ elsewhere.
So
\[
D(x)^2 = 1 \pmod{f(x)}.
\]
$D(x)$ can only have a finite number of roots in $F$.
Avoiding them we can find a value $a$ for $x$ such that $D(a) \not= 0$.
Now set $\theta = g(a)$.
Then the determinant
\begin{equation}
\label{eq:N5}
|\sigma_i \sigma_k (\theta) | \not= 0.
\end{equation}

Consider any linear relation $x_1 \sigma_1(\theta) + x_2 \sigma_2(\theta) + \cdots + x_n \sigma_n(\theta) = 0$ where the $x_i$ are in $F$.
Applying the automorphism $\sigma_i$ to it would lead to $n$ homogeneous equations for the $n$ unknowns $x_i$.
\eqref{eq:N5} shows that $x_i = 0$ and our theorem is proved.
\end{proof}


\subsection{Theorem on natural irrationalities}

Let $F$ be a field, $p(x)$ a polynomial in $F$ whose irreducible factors are separable, and let $E$ be a splitting field for $p(x)$.
Let $B$ be an arbitrary extension of $F$, and let us denote by $EB$ the splitting field of $p(x)$ when $p(x)$ is taken to lie in~$B$.
If $\alpha_1, \ldots, \alpha_s$ are the roots of $p(x)$ in $EB$, then $F(\alpha_1, \ldots, \alpha_s)$ is a subfield of $EB$ which is readily seen to form a splitting field for $p(x)$ in $F$.
By Theorem~\ref{theo:onze}, $E$ and $F(\alpha_1,\ldots,\alpha_s)$ are isomorphic.
There is therefore no loss of generality if in the sequel we take $E = F(\alpha_1,\ldots,\alpha_s)$ and assume therefore that $E$ is a subfield of $EB$.
Also, $E = B(\alpha_1, \ldots, \alpha_n)$.

Let us denote by $E \cap B$ the intersection of $E$ and $B$.
It is readily seen that $E \cap B$ is a field and is intermediate to $F$ and $E$.


\begin{theo}
\label{theo:29}
If $G$ is the group of automorphisms of $E$ over $F$, and $H$ the group of $EB$ over $B$, then $H$ is isomorphic to the subgroup of $G$ having $E \cap B$ as its fixed field.
\end{theo}


\begin{proof}
Each automorphism of $EB$ over $B$ simply permutes $\alpha_1, \ldots, \alpha_s$ in some fashion and leaves $B$, and hence also $F$, fixed.
Since the elements of $EB$ are quotients of polynomial expressions in $\alpha_1, \ldots, \alpha_s$ with coefficients in $B$, the automorphism is completely determined by the permutation it effects on $\alpha_1, \ldots, \alpha_s$.
Thus, each automorphism of $EB$ over $B$ defines an automorphism of $E = F(\alpha_1,\ldots,\alpha_s)$ which leaves $F$ fixed.
Distnict automorphisms, since $\alpha_1, \ldots, \alpha_s$ belong to $E$, have different effects on $E$.
Thus, the group $H$ of $EB$ over $B$ can be considered as a subgroup of the group $G$ of $E$ over $F$.
Each element of it leaves $E \cap B$ fixed since it leaves even all of $B$ fixed.
However, any element of $E$ which is not in $E \cap B$ is not in $B$, and hence would be moved by at least one automorphism of $H$.
It follows that $E \cap B$ is the fixed field of $H$.
\end{proof}


\begin{coro*}
If, under the conditions of Theorem~\ref{theo:29}, the group $G$ is of prime order, then either $H = G$ of $H$ consists of the unit element alone.
\end{coro*}




\section{Applications}








\end{document}
