\documentclass[11pt]{article}

\usepackage{tgpagella}
\linespread{1.1}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[normalem]{ulem}
\usepackage{textcomp}
\usepackage{hyperref}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\newtheorem{theo}{Theorem}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{lemm}[theo]{Lemma}
\newtheorem{coro}[theo]{Corollary}
\theoremstyle{definition}
\newtheorem{defi}[theo]{Definition}
\newtheorem{exam}[theo]{Example}

\newcommand{\kk}[1]{\mathbb{#1}}
\newcommand{\cc}[1]{\mathcal{#1}}

\def\eps{\varepsilon}
\def\empty{\varnothing}

\def\oint#1#2{(#1, #2)}
\def\cint#1#2{[#1, #2]}
\def\coint#1#2{[#1, #2)}
\def\ocint#1#2{(#1, #2]}

\def\ov#1{\overline{#1}}

\def\CC{\mathcal{C}}
\def\EE{\mathcal{E}}
\def\FF{\mathcal{F}}
\def\NN{\mathbf{N}}
\def\RR{\mathbf{R}}
\def\QQ{\mathbf{Q}}
\def\ZZ{\mathbf{Z}}

\author{Gunnar Þór Magnússon}
\date{\today}
\title{Real analysis}

\begin{document}

\maketitle


\section{Idea}

I read an article the other day that proposed to teach calculus by focusing on
$\FF(X)$, the algebra of real-valued functions on $X \subset \RR$.
That could be fun to do, so why not sketch out some ideas?
Topics to cover:

\begin{itemize}
\item
Construction of the real numbers.
Supremum and infimum.

\item
Sequences and convergence.
Compact sets.
Connected sets.

\item
Continuity of functions. Define $\CC^0(X) \subset \FF(X)$.

\item
Mean-value theorem.

\item
Integration. Darbeaux integral? A continuous function is integrable.

\item
Differentation. Definition of $\CC^1(X) \subset \CC^0(X)$.
Chain rule, product rule.

\item
Fundamental theorem of calculus.

\item
Definition of the algebra $\EE(X)$ of elementary functions on $X$:
Generated by polynomials, trigonometric functions, exponential, roots.
Differentiation of those functions is mechanical.

\item
Numerical analysis?
Taylor series?
\end{itemize}


\bibliographystyle{plain}
\bibliography{main}



\section{The numbers}

Let us begin somewhere close to the beginning.
We shall conspire to believe the \emph{natural numbers} exist.
Those are the numbers $0$, $1$, $2$, $\ldots$ we use to count, and we write
\[
	\NN = \{0, 1, 2, \ldots \}
\]
for the set they form.
We can build this set by giving ourselves the number $0$, and saying that
for any natural number $n$ there is always at least one more number $s(n)$.
The natural numbers are then the set of all the numbers we can build this way.

Given any two natural numbers $n$ and $m$ we can add them and get a new natural
number $n + m$.
In this context our mystery ``one more number'' above is $s(n) = n+1$.
This operation is commutative, so $n + m = m + n$.
It is also associative, which means that $(n + m) + k = n + (m + k)$ for all
natural numbers $n$, $m$ and $k$.
The number $0$ satisfies $n + 0 = 0 + n = n$ for all numbers $n$, and is the
only number that does so.

Given any two natural numbers $n$ and $m$ we can also multiply them and get a
new natural number $nm$.
This operation is also commutative, so $nm = mn$, and associative, so $n(mk) =
(nm)k$ for all numbers $n$, $m$ and $k$.
The number $1$ satisfies $1 \cdot n = n \cdot 1 = n$ for all $n$, and is the
only number that does so.

Given any two natural numbers $n$ and $m$, one of three things can happen:
Either $n = m$, or there is a natural number $k \not= 0$ such that $n = m + k$
or $m = n + k$.
If $n = m + k$ with $k \not= 0$ we say that $m$ is smaller than $n$ and write
$m < n$.
This defines a total order on the natural numbers, so for any two numbers $n$
and $m$ we either have $n < m$, $m < n$ or $n = m$.
We also write $n \leq m$ to mean that either $n < m$ or $n = m$.



\section{The real numbers}

We want to talk about functions of real variables, so we have to take some time
to say clearly what we mean by ``real numbers''.
Let us at least assume that we all agree that the natural numbers exist.
Those are the numbers $0$, $1$, $2$, $\ldots$ we use to count, and we write
\[
	\NN = \{0, 1, 2, \ldots \}
\]
for the set they form.
Closely related are the integers $\ZZ$, which include the negative natural
numbers $-1$, $-2$, and so on.
From these we get the rational numbers
\[
\QQ = \{ p / q \mid \text{$p, q \in \ZZ$ and $q \not= 0$} \},
\]
which are ratios of integers.

The natural numbers and positive rational numbers have been known since antiquity.
In most places it took more time for negative numbers to become
widespread, but we have known for a long time that the rational numbers do not
cover everything:

\begin{prop}
There is no rational number $x$ such that $x^2 = 2$.
\end{prop}

\begin{proof}
Suppose that $x = p/q$ were such a number.
We may assume here that $p$ and $q$ have no common factor; otherwise we cancel
those out.
Then $p^2 / q^2 = 2$ so $p^2 = 2 q^2$.
Therefore $p^2$ is even.
But then $p$ must also be even, so $p = 2r$.
Then $p^2 = 4r^2 = 2q^2$, so by canceling we get $q^2 = 2 r^2$, and so $q$ is
also even.
But this contradicts our assumption that $p$ and $q$ have no common factor.
\end{proof}


\section{Topology}


If $a$ and $b$ are extended real numbers we denote by
\[
	\oint ab = \{ x \in \RR \mid a < x < b \}
\]
the \emph{open interval} from $a$ to $b$.
Note that if $b < a$ then $\oint ab = \empty$.
Similarly we define the \emph{closed interval} from $a$ to $b$ by
\[
	\cint ab = \{ x \in \RR \mid a \leq x \leq b \},
\]
and we leave to the reader to imagine what the half-open intervals $\coint ab$ and
$\ocint ab$ are.

\begin{defi}
A subset $U \subset \RR$ is \emph{open} if for any $x \in U$ there
exists an open interval $\oint ab$ such that $x \in \oint ab \subset U$.
\end{defi}

Note that an open interval is open by this definition.
The next result says that open sets form what is called a topology.

\begin{prop}
\begin{enumerate}
\item
The empty set $\empty$ is open.

\item
The real line $\RR$ is open.

\item
If $(U_j)_{j \in J}$ is a collection of open sets, then their union 
$\bigcup_{j \in J} U_j$ is open.

\item
If $U_1, \ldots U_n$ are open sets, then their intersection $\bigcap_{j=1}^n
U_j$ is open.
\end{enumerate}
\end{prop}

\begin{proof}
\begin{enumerate}
\item
We have $(1,-1) = \empty$, so the empty set is open.

\item
If $x \in \RR$ then $x \in (x-1,x+1) \subset \RR$, so $\RR$ is open.

\item
If $x \in \bigcup_{j \in J} U_j$ then there is some $j \in J$ such that
$x \in U_j$.
Then there is an interval $\oint ab$ such that $x \in \oint ab \subset U_j$.
But then $\oint ab$ is also contained in the union, so it is open.

\item
If the intersection is empty there is nothing to prove.
Let thus $x \in \bigcap_{j=1}^n U_j$.
Then $x \in U_j$ for all $j$, so there are intervals $\oint{a_j}{b_j} \subset U_j$
that contain $x$.
Let $a = \max_j a_j$ and $b = \min_j b_j$.
Since $a_j < x$ for all $j$ we have $a < x$, and similarly we have $x < b$.
Then $x \in \oint ab$, and $\oint ab \subset \oint{a_j}{b_j} \subset U_j$ for all $j$,
so $\oint ab$ is contained in the intersection.
\end{enumerate}
\end{proof}



We will very often be interested in a small open interval around a given point.
If the point is $x$ we could write this as $\oint{x-r}{x+r}$ for some $r > 0$.
This gets a little tiresome after a while, so we introduce some notation along
with a name.


\begin{defi}
The \emph{open ball} around $x$ of radius $r$ is
\[
B(x, r)
= \{ y \in \kk R \mid |x - y| < r \}
= \oint{x-r/2}{x+r/2}.
\]
\end{defi}

Introducing this here is arguably overkill. This idea comes from the space
$\RR^n$ of $n$-tuples of real numbers, or more generally from metric spaces. It
rather useful however, and convenient when we focus on a point rather than on a
whole open set. It should be clear that an open ball is open, as it is
literally an open interval.



\subsection{Limits}


The whole point of analysis is to investigate questions of approximation.
Given a function and its value at a point, we would like to try to approximate
the values of the function close to that point.
Pulling at this thread will eventually lead us to derivatives.
Before we get there, we first have to say what we mean by approximations.

Suppose we have a function $f : X \to \RR$ defined on some set $X \subset \RR$.
Given a point $x_0$, which we'll say is in $X$ for now, we would like to know
if the nearby values of $f$ approximate the value $f(x_0)$.
We do this by considering the difference $|f(x) - f(x_0)|$, which is sometimes
called the error in the approximation.
If we can make this as small as we want by moving $x$ closer and closer to $x_0$,
then the successive approximations become better and better.

For technical reasons, it is useful to allow the situation when $x$ is not in $X$.
It may for example be possible to extend the function $f$ a little further so
its domain includes $x$, but a priori it is not clear this is doable.
As a slightly contrived example, consider the rational function
\[
f : \RR \setminus \{ 1 \} \to \RR,
\quad
f(x) = \frac{x^2 - 3x + 2}{x - 1}.
\]
It is not defined at $x_0 = 1$, but after developing the ideas in this section
we will see that we can assign it a limit there.
Here that is because the numerator factors as $x^2 - 3x + 2 = (x-1)(x-2)$ and
one factor cancels out against the denominator, so in fact $f(x) = x-2$ on all
of $\RR$. In general it may not be this clear that this happens, and it is
useful to allow for the greater flexibility.



\begin{defi}
A function $f : X \to \RR$ has a \emph{limit} $y$ at a point $x_0$ if for every
$\eps > 0$ there exists a $\delta > 0$ such that
\(
|f(x) - y| < \eps
\)
for all $x \not= x_0$ such that $|x - x_0| < \delta$.
\end{defi}

When the limit exists we write $\lim_{x \to x_0} f(x) = y$.
Morally this definition says that no matter how small of an error tolerance
$\eps$ we have, the function approximates the limit within that tolerance for
points close enough to $x_0$.

This definition of a limit is classical.
There is an equivalent definition in terms of open sets that is more abstract
but sometimes convenient to work with.
They both have their place;
if we want to prove something about limits that involves properties of the real
numbers, like addition or multiplication, it is very likely we want to use the
epsilon-delta definition;
while if we want to prove something more nebulous, like that the composition
of limits behaves well, we may want to use the more abstract version.

\begin{prop}
Let $f : X \to \RR$ be a function.
The following are equivalent:
\begin{itemize}
\item
The function $f$ has the limit $y$ at $x_0$.

\item
For every open set $U$ that contains $y$, there exists an open set $V$ that
contains $x_0$ such that $f(V \setminus \{ x_0 \}) \subset U$.
\end{itemize}
\end{prop}


\begin{proof}
Suppose $\lim_{x \to x_0} f(x) = y$.
Let $U$ be an open set that contains $y$, and $B(y,\eps) \subset U$ an open
interval around $y$ inside $U$.
Then the open set $B(x_0, \delta)$ contains $x_0$ and satisfies $f(B(x_0,
\delta) \setminus \{x_0\}) \subset B(y, \eps) \subset U$.

Suppose then that the open set condition holds.
If $\eps > 0$ is given, then $B(y, \eps)$ is an open interval that contains $y$.
Then there is an open set $V$ that contains $x$ such that $f(V \setminus
\{x_0\}) \subset B(y, \eps)$.
Since $V$ is open and contains $x_0$ there is a $\delta > 0$ such that $B(x_0,
\delta) \subset V$.
But this means exactly that $|f(x) - y| < \eps$ for all $x$ such that $|x -
x_0| < \delta$.
\end{proof}


\begin{theo}[Limits are unique]
Let $f : X \to \RR$ be a function that has a limit $y$ at $x_0$.
If $z$ is another limit of $f$ at $x_0$, then $y = z$.
\end{theo}

\begin{proof}
Suppose that $y \not= z$ and let $\eps = |y - z|/2 > 0$.
Since $y$ is a limit of $f$ at $x_0$ there exists $\delta_y > 0$ such that
$|f(x) - y| < \eps$ whenever $|x - x_0| < \delta_y$.
Similarly, a $\delta_z > 0$ also exists so $|f(x) - z| < \eps$ holds when
$|x - x_0| < \delta_z$.
Let now $\delta = \min\{\delta_y, \delta_z\}$.
Then
\begin{align*}
2\eps = |y - z| 
&= |(y - f(x)) + (f(x) - z)| 
\\
&\leq |f(x) - y| + |f(x) - z|
< \eps
\end{align*}
for $x$ such that $|x - x_0| < \delta$.
Thus $2 \eps < \eps$, which is a contradiction.
\end{proof}



We can already say that a couple of functions always have limits at any point.

\begin{prop}
\begin{itemize}
\item
Let $f : X \to \RR$ be the constant function $f(x) = c$ for some $c \in \RR$.
Then $\lim_{x \to x_0} f(x) = c$ for any $x_0$.

\item
Let $f : X \to \RR$ be the identity function $f(x) = x$.
Then $\lim_{x \to x_0} f(x) = x_0$ for any $x_0$.
\end{itemize}
\end{prop}

\begin{proof}
First let $f(x) = c$ be a constant function.
Let $U$ be an open set around $c$.
If $V$ is any open set that contains $x_0$, then $f(V \setminus \{x_0\}) = \{ c
\} \subset U$.

Next let $f(x) = x$ be the identity function.
If $\eps > 0$, we let $\delta = \eps$.
For any $x$ such that $|x - x_0| < \delta$ we then have
$|f(x) - x_0| = |x - x_0| < \delta = \eps$.
\end{proof}


Before we go further we should see a couple of examples of functions that don't
have limits.

\begin{exam}
Let $f : \RR \to \RR$ be defined as
\[
f(x) = \begin{cases}
0 & \text{if $x \not= 0$},
\\
1 & \text{if $x = 0$}.
\end{cases}
\]
Then $f$ has a limit at $x_0 = 0$, but the limit is not equal to $f(x_0)$!
Let $\eps > 0$ be given.
If we pick any $\delta > 0$ then
\(
|f(x)| = 0
\)
for all $x \not= 0$ such that $|x| < \delta$, so 
\[
\lim_{x \to 0} f(x) = 0 \not=1 = f(0).
\]
\end{exam}

\begin{exam}
Let $f : \RR \to \RR$ be defined as
\[
f(x) = \begin{cases}
1 & \text{if $x > 0$},
\\
0 & \text{if $x \leq 0$}.
\end{cases}
\]
Then $f$ does not have a limit at $0$.
Suppose it did and the limit was $y$.
Let $\eps = \frac12$.
For any $\delta > 0$ we consider $x$ such that $|x| < \delta$.
If $0 < x < \delta$ we then have $|y - f(x)| = |y - 1| < \frac12$.
If $-\delta < x < 0$ we also have $|y - f(x)| = |y| < \frac12$.
But then
\[
1
= |1 - y + y|
\leq |1 - y| + |y|
< 1,
\]
which is absurd.
\end{exam}

We can prove that the function in this example has limits at all other points
than $0$. Using similar staircase constructions we could, for example, create a
function that has no limits at each integer. In fact a function doesn't need to
have limits at any points at all.


\begin{exam}
Let $f : \RR \to \RR$ be the function defined by
\[
f(x) = \begin{cases}
1 & \text{if $x$ is rational},
\\
0 & \text{if $x$ is irrational}.
\end{cases}
\]
Then $f$ has no limit at any point:
Suppose it had a limit $y$ at $x_0$, and let $\eps = \frac12$.
For any $\delta > 0$ the open set $B(x_0, \delta)$ contains both a
rational number $r$ and an irrational number $x$.
We then have $|f(r) - y| = |1 - y| < \frac12$ and
$|f(x) - y| = |y| < \frac12$.
But then
\[
1 = |1 - y + y|
\leq |1 - y| + |y|
< 1,
\]
which is again absurd.
\end{exam}


The following lemma is very useful when doing epsilon-delta proofs.
Often we don't end up with a clean bound where something is less than
$\eps$, but instead less than $4\eps$ or $\eps^2$ or something similar.
The lemma says this is fine; we just have to be able to say that the bound
we come up with tends to zero at zero.


\begin{lemm}
Let $f : X \to \RR$ be a function.
Let $g : I \to \RR$ be a function from an interval $I$ around $0$ such that
$\lim_{x \to 0} g(x) = 0$.
If there is a $y \in X$ such that for every $\eps > 0$ there is a $\delta > 0$
such that
\[
|f(x) - y| \leq g(\eps)
\]
for all $x \in B(x_0, \delta)$, then $\lim_{x \to x_0} f(x) = y$.
\end{lemm}


\begin{proof}
Let $\eps > 0$.
There is a $\delta_g > 0$ such that $|g(x)| < \eps$ for all $x \not= 0$ such that
$|x| < \delta_g$.
Let $0 < \eps_f < \min\{\eps, \delta_g\}$.
Then there is a $\delta > 0$ such that
\[
|f(x) - y| \leq g(\eps_f) < \eps
\]
for all $x \not= x_0$ such that $|x - x_0| < \delta$.
\end{proof}


We need another minor lemma to bootstrap our way to an important result.


\begin{lemm}
The function $f(x) = x^2$ tends to $0$ at $0$.
\end{lemm}

\begin{proof}
Let $U$ be an open set that contains zero.
Then we can find an $0 < \eps < 1$ such that $B(0, \eps) \subset U$.
If $|x| < 1$ then $|x^2| \leq |x|$, so $f(B(0, \eps) \setminus \{0\}) \subset
B(0, \eps) \subset U$, and $B(0, \eps)$ is an open set that contains $0$.
\end{proof}


When limits exist, they play nicely with the addition and multiplication
operations on $\FF(X)$.

\begin{theo}
Let $f,g : X \to \RR$ be functions that both have a limit at $x_0$.
Then
\begin{align*}
\lim_{x \to x_0}(f(x) + g(x)) &= \lim_{x \to x_0} f(x) + \lim_{x \to x_0} g(x),
\\
\lim_{x \to x_0} f(x) g(x) &= \lim_{x \to x_0} f(x) \cdot \lim_{x \to x_0} g(x),
\\
\lim_{x \to x_0} \frac{1}{g(x)} &= 
\frac{1}{\lim\limits_{x \to x_0} g(x)},
\end{align*}
provided that $\lim\limits_{x \to x_0} g(x) \not= 0$ in the last equality.
\end{theo}


Recall that we identify an element $c \in \RR$ with the corresponding constant
function $c : X \to \RR$ when it is conventient for us.
Doing that above yields the also helpful equality $\lim_{x \to x_0} c f(x) = c
\lim_{x \to x_0} f(x)$.

\begin{proof}
Let's write $\lim_{x \to x_0} f(x) = y$ and $\lim_{x \to x_0} g(x) = z$ for short.
Let $\eps > 0$.
Then there is a $\delta_f > 0$ such that $|f(x) - y| < \eps$ when $|x - x_0| <
\delta_f$.
Similarly there is a $\delta_g > 0$ such that $|g(x) - z| < \eps$ when
$|x - x_0| < \delta_g$.
We set $\delta = \min\{\delta_f, \delta_g\}$.

For the first equality, note that
\begin{align*}
|f(x) + g(x) - (y + z)|
&= |(f(x) - y) + (g(x) - z)|
\\
&\leq |f(x) - y| + |g(x) - z|
< 2\eps
\end{align*}
for $x$ such that $|x - x_0| < \delta$.

For the second equality, we have
\[
(f(x) - y)(g(x) - z)
= f(x) g(x) - f(x) z - y g(x) + yz
\]
by expanding out the parentheses, so
\[
f(x) g(x)
= (f(x) - y)(g(x) - z) + f(x) z + y g(x) - yz.
\]
We will evaluate the limits of the terms on the right-hand side one by one and
add them, which we are allowed to do by the first equality we proved.
Note that
\[
|(f(x) - y)(g(x) - z)|
= |f(x) - y| |g(x) - z|
< \eps^2
\]
for all $x \in B(x_0, \delta)$, so the limit of the first term as $x \to x_0$
is $0$. We also have
\[
|f(x) z - yz| = |z| |f(x) - y| < |z| \eps,
\]
so the limit of the second term is $yz$.
Similarly the limit of the third term is also $yz$.
The fourth term is constant, so its limit is simply $yz$ as well.
Taken all together, we get
\begin{align*}
\lim_{x \to x_0} f(x) g(x)
&= \lim_{x \to x_0} (f(x) {-} y)(g(x) {-} z) 
{+}\!\! \lim_{x \to x_0} f(x) z 
{+}\!\! \lim_{x \to x_0} y g(x) 
{-}\!\! \lim_{x \to x_0} yz
\\
&= 0 + yz + yz - yz = yz.
\end{align*}

For the third equality, we have
\[
\biggl|\frac{1}{g(x)} - \frac{1}{z}\biggr|
= \biggl|\frac{g(x) - z}{g(x) z}\biggr|
< \frac{\eps}{|z| |g(x)|}
\]
for all $x \in B(x_0, \delta_g)$.
Let $\epsilon_g = |z|/2$ and let $\delta' > 0$ be such that
$|g(x) - z| < \epsilon_g$ for all $x \in B(x_0, \delta')$.
Then
\[
|g(x)| = |g(x) - z + z| \geq ||g(x) - z| - |z|| > |z| - |z|/2 = |z|/2
\]
for all such $x$.
If $\delta = \min \{\delta_g, \delta' \}$ we then have $1/|g(x)| \leq 2/|z|$
for $x \in B(x_0, \delta)$, and we get
\[
\biggl|\frac{1}{g(x)} - \frac{1}{z}\biggr|
< \frac{\eps}{|z| |g(x)|}
\leq \frac{2}{|z|^2} \eps.
\qedhere
\]
\end{proof}


We get the function $x \mapsto x^n$ by simply multiplying the identity function
by itself $n$ times.
As a consequence of the above theorem, we can calculate the limits of
any polynomial $p(x) = a_n x^n + \cdots + a_1 x + a_0$:
\[
\lim_{x \to x_0} p(x) = p(x_0).
\]
The same holds true for a rational function of the form $p(x) / q(x)$, where $p$
and $q$ are polynomials, for points where $q(x) \not= 0$.
This property is our first glimpse of \emph{continuity}, which is the cornerstone
of analysis.



\subsection{Continuity}


In the last section we saw that polynomials have the very convenient property
that $\lim_{x \to x_0} p(x) = p(x_0)$ for every $x_0$.
Beyond making limits of polynomials easy to calculate, it hints at something
fundamental: any value of a polynomial can be approximated by its nearby values.
In the examples of the previous section we saw that one reason limits can fail
to exist is that the function jumps at a point; think of the function that was
zero everywhere except at $0$.
When limits always exist, there can be no such jump; each value flows into the
nearby ones; when we graph the function, we can do so with an unbroken curve.
Analysis is all about approximations, and this property is important enough to
merit its own definition.


\begin{defi}
A function $f : X \to \RR$ is \emph{continuous at $x_0 \in X$} if 
\[
\lim_{x \to x_0} f(x) = f(x_0).
\]
The function is \emph{continuous} if it is continuous at all points in $X$.
\end{defi}

If we unpack the definition, we see that $f$ is continuous at $x_0$ if for
every $\eps > 0$ there exists a $\delta > 0$ such that $|f(x) - f(x_0)| < \eps$
for all $x$ such that $|x - x_0| < \delta$.
(The definition of a limit includes $x \not= x_0$, but $|f(x_0) - f(x_0)| = 0 <
\eps$.)
Rewriting this a little, this is the same as saying that for every $\eps > 0$
there exists a $\delta > 0$ such that $f(B(x_0, \delta)) \subset B(f(x), \eps)$.
Similar to limits, there is a more abstract version of continuity that is also
useful.


\begin{prop}
Let $f : X \to \RR$ be a function.
The following are equivalent:
\begin{itemize}
\item
The function is continuous.

\item
For every open set $U$ the set $f^{-1}(U)$ is open.
\end{itemize}
\end{prop}

\begin{proof}
Suppose $f$ is continuous and let $U$ be an open set.
Let $x_0 \in f^{-1}(U)$.
Let $\eps > 0$ be such that $B(f(x), \eps) \subset U$.
Then there is a $\delta > 0$ such that $f(B(x_0, \delta)) \subset B(f(x), \eps)
\subset U$.
But this means that $B(x_0, \delta) \subset f^{-1}(U)$ is an open ball that
contains $x_0$, so $f^{-1}(U)$ is open.

Suppose now the open set condition holds.
Let $x_0 \in X$ and $\eps > 0$.
The set $B(f(x), \eps)$ is open, so $f^{-1}(B(f(x), \eps))$ is also open.
It also contains $x_0$, so there is a $\delta > 0$ such that $B(x_0, \delta)
\subset f^{-1}(B(f(x), \eps))$.
But then $f(B(x_0, \eps)) \subset B(f(x), \eps)$, so $f$ is continuous.
\end{proof}


\begin{coro}
\label{continuous-fns-algebra}
Let $f, g : X \to \RR$ be functions that are continuous at $x_0$.
Then $f + g$, $f - g$, $f \cdot g$ and $f/g$ are all continuous at $x_0$, the
last one provided that $g(x_0) \not= 0$.
\end{coro}

We also know that constant functions and polynomials are continuous, because
we calculated their limits at every point, so some continuous functions exist
on every set $X \subset \RR$.


\begin{defi}
The set of continuous functions on $X$ is denoted by $\CC^0(X) \subset \FF(X)$.
By Corollary~\ref{continuous-fns-algebra} this is a subalgebra of $\FF(X)$.
\end{defi}


\begin{prop}
Let $X$ and $Y$ be subsets of $\RR$ and let $f : X \to \RR$ and 
$g : Y \to \RR$ be continuous functions.
If $f(X) \subset Y$ then $g \circ f : X \to \RR$ is continuous.
\end{prop}

\begin{proof}
If $U \subset \RR$ is open then $g^{-1}(U) \subset Y$ is open because $g$ is
continuous.
Then $f^{-1}(g^{-1}(U)) = (g \circ f)^{-1}(U)$ is also open.
\end{proof}




\subsection{Sequences}


It can be more comfortable to work with points instead of entire sets.
By picking points in a given set we can probe it in different ways.
Doing so can reveal properties of the original set, and can be a convenient way
of working with topological notions.
To formalize this idea of picking points we introduce sequences.


\begin{defi}
A \emph{sequence} in a set $X \subset \kk R$ indexed by a set $I \subset \NN$
is a collection $(x_n)_{n \in I}$ of points $x_n \in X$.
\end{defi}

Stated slightly differently, a sequence is just a function $I \to X$ where
$n \mapsto x_n$.
The notation above is convenient though, and we will see situations where
thinking of sequences as functions would get cumbersome.

A sequence can be finite if the index set $I$ is finite. If $I = \{n_1, n_2,
\ldots, n_m\}$ then a sequence indexed by $I$ is just a finite collection of
points $x_{n_1}$, $x_{n_2}$, $\ldots$, $x_{n_m}$.
A sequence can also be infinite, like say $x_n = 2^n$ in $\kk R$.
We often think of sequences as being indexed by the positive natural numbers
$1$, $2$, $3$, $\ldots$~.
In that case we sometime skip writing the index set and just write $(x_n)$.

The sequence $x_n = 2^n$ we talked about above grows without bounds; its terms
get larger and larger as $n$ grows and eventually become larger than any fixed
number. Other sequences, like $x_n = 1/n$ get closer and closer to some number,
in this case zero. Yet other sequences, like $x_n = (-1)^n$ flip around and
settle on no one destination. The notion of convergence captures what goes on here.


\begin{defi}
A sequence $(x_n)_{n \in I}$ in $X$ \emph{converges to $x$} in $X$ if for every
open set $U \subset X$ that contains $x$, there is an $N \in I$ such that
$x_n \in U$ for every $n \geq N$.
\end{defi}

If $(x_n)_{n \in I}$ converges to $x$ we write $\lim_{n \in I} x_n = x$.
When $I$ is an infinite set we very often write $\lim_{n \to \infty} x_n = x$.
In either case we say that $x$ is the \emph{limit} of the sequence.

The definition of convergence of a sequence is often stated as there being an
$N \in \NN$ such that $x_n \in U$ for all $n \geq N$.
Under that version, it would seem that finite sequences converge to any
possible value:
Suppose for example that $(x_1)$ is a sequence of one element.
Given any $x \in \RR$ and any open set $U$ around it, the statement $x_n \in U$
for all $n \geq 2$ is vacuous -- there is no such element $x_n$ in the sequence
-- and vacuous statements are true.
Therefore the sequence $(x_1)$ converges to $x$, which was arbitrary.

As a first sanity check of our version we thus prove that finite sequences
converge to their last element.


\begin{prop}
Let $I \subset \NN$ be finite and $(x_n)_{n \in I}$ a sequence.
Then $(x_n)$ converges to $x_{\max I}$.
\end{prop}

\begin{proof}
Let $U \subset X$ be an open neighborhood around $x_{\max I}$.
Then $\max I \in I$ is such that $x_n \in U$ for all $n \geq \max I$.
\end{proof}


\begin{theo}[Limits are unique]
Let $(x_n)_{n \in I}$ be a sequence that converges to $x$.
If the sequence also converges to $y$, then $x = y$.
\end{theo}

\begin{proof}
We will prove the contrapositive, that if $x \not= y$ and $(x_n)$ converges
to $x$ then it does not converge to $y$.
Suppose then that $x \not= y$.
Let $U = B(x, |x-y|/2)$ be an open neighborhood around $x$, and
$V = B(y, |x-y|/2)$ be an open neighborhood around $y$.
Note that $U \cap V = \empty$:
If $z \in U \cap V$ then $|x - z| < |x - y|/2$ and $|y - z| < |x - y|/2$.
But then
\begin{align*}
|x - y|
= |(x - z) - (y - z)|
&\leq |x - z| + |y - z|
\\
&< |x - y|/2 + |x - y|/2
= |x - y|,
\end{align*}
which is a contradiction.
As $(x_n)$ converges to $x$ there is an $N(x)$ such that $x_n \in U$
for all $n \geq N(x)$.
But then there cannot be an $N(y)$ such that $x_n \in V$ for all $n \geq N(y)$,
so $(x_n)$ cannot converge to $y$.
\end{proof}



\begin{theo}
Let $(x_n)_{n \in I}$ and $(y_n)_{n \in I}$ be a sequences in $X$
that converge to $x$ and $y$, respectively, and let $c \in \RR$.
Then
\begin{align*}
\lim_{n \in I}(-x_n) &= -x,
\\
\lim_{n \in I}(x_n + y_n) &= x + y,
\\
\lim_{n \in I}(c x_n) &= c x,
\\
\lim_{n \in I} (x_n y_n) &= x y,
\\
\lim_{n \in I} \biggl( \frac{x_n}{y_n} \biggr) &= \frac{x}{y},
\end{align*}
provided that $y \not= 0$ in the last equality.
\end{theo}

\begin{proof}
First let $U$ be a neighborhood of $-x$.
Then there exists $r > 0$ such that $B(-x,r) \subset U$.
We find $N$ such that $x_n \in B(x,r)$ for all $n \geq N$,
that is so that $|x_n - x| < r$ for all $n \geq N$.
But then $|-x_n - (-x)| = |-x_n + x| = |x_n - x| < r$ for all $n \geq N$,
so $-x_n \in B(-x,r) \subset U$ for all $n \geq N$.


Second, let $U$ be a neighborhood of $x + y$.
Then there exists $r > 0$ such that $B(x+y, r) \subset U$.
As $B(x, r/2)$ and $B(y, r/2)$ are open neighborhoods around $x$ and $y$,
there exists $N \in I$ such that $x_n \in B(x, r/2)$ and $y_n \in B(y, r/2)$
for all $n \geq N$.
But then
\begin{align*}
| x_n + y_n - (x + y) |
&= | (x_n - x) + (y_n - y) |
\\
&\leq |x_n - x| + |y_n - y|
< r/2 + r/2 = r
\end{align*}
for all $n \geq N$, so $x_n + y_n \in B(x+y,r) \subset U$ for all $n \geq N$.


Third, note first that if $c = 0$ then $(c x_n)$ clearly converges to $0$, and
$0 \cdot x = 0$. Suppose then that $c \not= 0$. Let $U$ be a neighborhood of
$cx$ and find $r > 0$ such that $B(cx,r) \subset U$. Then let $N$ be such
that $x_n \in B(x,r/|c|)$ for all $n \geq N$.
Then
\[
|cx_n - cx| = |c| |x_n - x| < |c| r /|c| = r
\]
so $cx_n \in B(cx,r) \subset U$ for all $n \geq N$.
\end{proof}


\section{Differentation}

Differentation as the best linear approximation of a function.
Let's consider a continuous function $f : U \to \kk R$ and a point $x_0 \in U$.
We can approximate $f$ around $x_0$ by a linear function of the form $\ell_m(x)
= f(x_0) + m (x - x_0)$, where $m$ is a real number. We have defined $\ell_m$
so that $\ell_m(x_0) = f(x_0)$, but otherwise different values of $m$ may
result in functions that approximate $f$ better or worse near $x_0$.
Is there one that approximates $f$ best of all?

We haven't said what we mean by best linear approximation.
Let's see if we can figure something out by doing a trick that appears all over
math: pretend we already know the answer.
Suppose we have a continuous function $f$ like above
and we have \emph{the best} linear approximation to it at $x_0$ and that
it is $\ell_m(x) = f(x_0) + m(x - x_0)$.
If $g(x) = f(x_0) + k(x - x_0)$ is some other linear approximation to $f$ it
should then be \emph{worse}.
For small enough $\delta$ we should then have
$$
|f(x) - f(x_0) - m(x - x_0)|
\leq |f(x) - f(x_0) - k(x - x_0)|
$$
for all $x$ with $|x - x_0| < \delta$.




\end{document}
